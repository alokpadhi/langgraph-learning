{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32d9dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alokpadhi/langgraph-learning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from operator import add\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25833389",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefa069",
   "metadata": {},
   "source": [
    "### Short term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9febf3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State with conversation buffer\n",
    "class ConversationState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add]\n",
    "    buffer_size: int  # Max messages to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ffd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27d3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant with memory of the conversation.\n",
    "Reference previous messages when relevant to provide continuity.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7afbf47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36f994f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40ecb418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state: ConversationState) -> dict:\n",
    "    \"\"\"Chat node with automatic buffer management\"\"\"\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Processing with {len(state['messages'])} messages in buffer\")\n",
    "        \n",
    "        # Trim messages if buffer is too large\n",
    "        buffer_size = state.get(\"buffer_size\", 10)\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "        if len(messages) > buffer_size:\n",
    "            # Keep most recent messages\n",
    "            messages = messages[-buffer_size:]\n",
    "            logger.info(f\"Trimmed buffer to {buffer_size} messages\")\n",
    "        \n",
    "        # Invoke LLM\n",
    "        response = chain.invoke({\"messages\": messages})\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [response]\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat error: {e}\")\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {str(e)}\")]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb30c354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x74f42f04e950>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(ConversationState)\n",
    "workflow.add_node(\"chat\", chat_node)\n",
    "workflow.set_entry_point(\"chat\")\n",
    "workflow.add_edge(\"chat\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9783ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72049795",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"conversation_memory.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "conversation_agent = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bede91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dec52f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(conversation_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8fa5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_memory(\n",
    "    message: str,\n",
    "    session_id: str = \"default\",\n",
    "    buffer_size: int = 10\n",
    ") -> dict:\n",
    "    \"\"\"Chat with conversation buffer\"\"\"\n",
    "    \n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": f\"conv-{session_id}\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "        \"buffer_size\": buffer_size\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = conversation_agent.invoke(initial_state, config=config)\n",
    "        \n",
    "        final_message = result[\"messages\"][-1]\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": final_message.content,\n",
    "            \"total_messages\": len(result[\"messages\"])\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat failed: {e}\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6335875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 13 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CONVERSATION BUFFER DEMO\n",
      "============================================================\n",
      "\n",
      "--- Turn 1 ---\n",
      "User: Hi, my name is Alice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 15 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Nice to meet you properly, Alice! I remember our conversation earlier. You've been open with me about your job as a software engineer and your interest in AI and machine learning. Is there anything on your mind that you'd like to chat about or ask for advice on? I'm here to listen and help if I can.\n",
      "(Buffer size: 14 messages)\n",
      "\n",
      "--- Turn 2 ---\n",
      "User: What's my name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 17 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Your name is Alice! We've already established that, but it's nice to remind ourselves. You're having a bit of fun with our conversation, aren't you? Don't worry, I won't hold it against you!\n",
      "(Buffer size: 16 messages)\n",
      "\n",
      "--- Turn 3 ---\n",
      "User: I work as a software engineer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 19 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You mentioned earlier that you work as a software engineer. What specific areas or technologies are you interested in within software engineering? Are you looking to learn more about AI and machine learning in the context of your job, or do you have other interests in mind?\n",
      "\n",
      "Also, I don't want to keep repeating what you've already said, so I'll try to dive a bit deeper into our conversation. What motivated you to pursue a career in software engineering?\n",
      "(Buffer size: 18 messages)\n",
      "\n",
      "--- Turn 4 ---\n",
      "User: What do I do for work?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 21 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Déjà vu! We've been here before, Alice. You mentioned earlier that you work as a software engineer. If you'd like to explore more about your job or discuss any related topics, I'm here to help.\n",
      "\n",
      "However, if you're genuinely curious about what a software engineer does, I can try to provide some general information. Software engineers design, develop, and test software programs for various industries and applications. They work on writing code, debugging, and collaborating with teams to deliver high-quality software solutions.\n",
      "\n",
      "But since we've already had this conversation before, I'm curious - is there something specific you'd like to talk about related to your job or work in general?\n",
      "(Buffer size: 20 messages)\n",
      "\n",
      "--- Turn 5 ---\n",
      "User: I'm interested in AI and machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 23 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: We've been here before, Alice! You mentioned earlier that you're interested in AI and machine learning. That's a fascinating area of study!\n",
      "\n",
      "Since we've had this conversation before, I want to make sure I understand what draws you to AI and machine learning. Are you looking to apply these concepts to your current work as a software engineer or explore them in a different context?\n",
      "\n",
      "Also, are there any specific aspects of AI and machine learning that interest you the most? For example, natural language processing, computer vision, or perhaps reinforcement learning?\n",
      "(Buffer size: 22 messages)\n",
      "\n",
      "--- Turn 6 ---\n",
      "User: What are my interests?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 25 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You mentioned earlier that you're interested in AI and machine learning. That's a great starting point!\n",
      "\n",
      "Let me try to recap our conversation so far: You work as a software engineer and have expressed interest in AI and machine learning.\n",
      "\n",
      "Is there anything else you'd like to add or discuss about your interests? Or would you like to explore topics related to AI and machine learning further?\n",
      "\n",
      "(By the way, I'll keep in mind that we've had this conversation before, but feel free to correct me if my memory is a bit fuzzy!)\n",
      "(Buffer size: 24 messages)\n",
      "\n",
      "--- Turn 7 ---\n",
      "User: do you remember my name?\n",
      "Agent: You mentioned earlier that your name is Alice. I'm glad I could recall it correctly! Would you like to continue our conversation with a fresh start or revisit something we discussed earlier, Alice?\n",
      "(Buffer size: 26 messages)\n"
     ]
    }
   ],
   "source": [
    "session = \"user-123\"\n",
    "    \n",
    "conversations = [\n",
    "    \"Hi, my name is Alice\",\n",
    "    \"What's my name?\",\n",
    "    \"I work as a software engineer\",\n",
    "    \"What do I do for work?\",\n",
    "    \"I'm interested in AI and machine learning\",\n",
    "    \"What are my interests?\",\n",
    "    \"do you remember my name?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONVERSATION BUFFER DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, msg in enumerate(conversations, 1):\n",
    "    print(f\"\\n--- Turn {i} ---\")\n",
    "    print(f\"User: {msg}\")\n",
    "    \n",
    "    result = chat_with_memory(msg, session_id=session, buffer_size=10)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(f\"Agent: {result['response']}\")\n",
    "        print(f\"(Buffer size: {result['total_messages']} messages)\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bb4e036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'response': \"You mentioned earlier that your name is Alice. I'm glad I could recall it correctly! Would you like to continue our conversation with a fresh start or revisit something we discussed earlier, Alice?\",\n",
       " 'total_messages': 26}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7974a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3e65f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70c024ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = SystemMessage(\n",
    "    content=\"You are a helpful assistant with memory of the conversation. Reference previous messages when relevant.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb2477a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_trimming(state: ConversationState) -> dict:\n",
    "    \"\"\"Chat node with automatic message trimming\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get messages\n",
    "        messages = list(state[\"messages\"])\n",
    "        \n",
    "        logger.info(f\"Processing with {len(messages)} messages\")\n",
    "        \n",
    "        # Add system message if not present\n",
    "        if not messages or not isinstance(messages[0], SystemMessage):\n",
    "            messages = [SYSTEM_MESSAGE] + messages\n",
    "        \n",
    "        # Use trim_messages to intelligently manage history\n",
    "        # This keeps system message, trims middle, keeps recent messages\n",
    "        trimmed_messages = trim_messages(\n",
    "            messages,\n",
    "            max_tokens=1000,  # Keep last ~1000 tokens\n",
    "            strategy=\"last\",  # Keep most recent messages\n",
    "            token_counter=llm,  # Use LLM's token counter\n",
    "            include_system=True,  # Always keep system message\n",
    "            allow_partial=False,  # Don't split messages\n",
    "            start_on=\"human\"  # Start on human message for context\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Trimmed to {len(trimmed_messages)} messages\")\n",
    "        \n",
    "        # Invoke LLM with trimmed messages\n",
    "        response = llm.invoke(trimmed_messages)\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [response]\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat error: {e}\")\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {str(e)}\")]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68cb9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74183ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x74f42f0af2d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(ConversationState)\n",
    "workflow.add_node(\"chat\", chat_with_trimming)\n",
    "workflow.add_edge(START, \"chat\")\n",
    "workflow.add_edge(\"chat\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbe74f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"conversation_memory_new.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "conversation_agent = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62bd44d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAADqCAIAAADF80cYAAAQAElEQVR4nOydCVxU5d7Hn3NmA2YY9h1BwBU1MTExF3Ihfb2RWHT1at1baqW5kKY3K61Q+5jXbFPLrGyxzNI0KcvUTFQ0RUTDDWVHFpFlGJhhtnPO+5wZHBZnf87oGThfdD4zz3nOmTO/8yz/Z/3zKYoCHI7CBxwIcPIhwcmHBCcfEpx8SHDyIYEqX/Gllus58oZbGo2aJHUAtLOCMBxQJKAw+q99SOt7PqB0HUIM4DxAEp2/hSekCA3WKRDnYxRJdTrdcFmAtd0JJgCUtuPVRLjIDRdLeZH9xAMe9AQIYI7ZfTlH5JdONjQ36uAP4PNxnhATinBaC6Ltahiu/3k4hpFU+xDDe/jjSR3VPqQ1XABIbeev44twnZrsFIjz9Y+nYzCPhxEE1f6p4AKM1Hb4Cp6ARxKkVkNq1BS8BzcxLypWMnaaP7Afu+U7d0SWc7ieIEBguFv8BL+I/iLgyjTXUcczam4UKgkNGTVQMvHfQXadbp98X60pbWkiYhO8x0z1BV2LK6ebT+6/RRFg9ptRML/biB3yfby00L+H6Im0cNB1ydxVe/lM44h/+Mc95GVLfFvl27SkYNw/g2MTJKAb8NHSgpmvRHn58azGtEm+zS8VPPdWL4Eb6D5sfaVo6Di/oUlW0iAOrLHl5aLx04O7lXaQ59ZGnzlU21htJW1Zke+r1aUB4aJ+w7pFnu3E8EkBOz8othzHknxnD8uUzcTjC8NAt+T+cVI3D/zHjRUW4liSL/tQfewDNlVAXZXUtIiq4hYLEczKd+FoEyCoxMf9QDdGLMUlUsHezZXmIpiV7/zxhqBId3B3SUpKqqiosPeswsLCRx55BDiHgaOk1aVmE6BZ+Zplmviku5r0qqqqGhoagP1cvnwZOI34CT6w+Vx2zbSCpntcrp9T4Dge0c8p7VloaX733Xe//PJLaWlpVFRUQkLCvHnzcnNz586dC49OmTIlMTFxw4YNME3t3r07Ozu7srIyOjo6JSUlNTXVcIXx48fPmTPnyJEj8Kynnnpq+/bt9O+Mj1+8ePHMmTMB04g88LxjjRF9TORF0/IVX1YIRBhwDjt37ty2bduLL744cuTIo0ePbt68WSwWP/PMM++//z4M3LdvX1gYXddDBaFwr732GoZhJSUl69atCwkJgafAQwKBYO/evQ888AAUcejQoTDCwYMH4fMAzsHLT9hYrzF5yLR88jqtyMN6k8Uxzp07Fxsbayitpk6dOmzYMKVSeWe0tWvXKhSK0NBQoE9ZGRkZJ0+eNMgH9fLy8lq6dCm4K0i8+RWF9sinVZNCofUGiWMMHjx448aNq1atGjJkyJgxY8LDTfdBwDwO02lWVhbM44YQQ6o0AB8AuFt4eOKE1nTzw7R8JEVglLNS34wZM2BuzczMTE9P5/P5sLZdtGhRQEBAhxsgybS0NI1Gs2DBApj0PD09Z8+e3T6CUCgEdw0MA2ZKMtPyidwEaiUJnAOslKbqKSoqOnPmzNatW5ubm9977732ca5evXrp0qWPPvoIFnCGkKampsDAQHAvaGkicdy0fqblE0v5slolcA6wjO/fv39MTEy0HqgLrAc6xZHJZPDVqFeRHngKuBc0ybQCN9NCmS7gIvqLtSpnpb4DBw4sW7bs2LFjjY2NJ06cgPYHLA1heM+ePeHroUOHLl68CGWF+RpaJHK5HFa769evh/YNNAxNXjAiIqK2thZW4sZSkllkNRqpt+mizLR8AxIkhI6qr9YCJ7BixQqozpIlS6D5tnr1amjlQesEhsM6JDk5ecuWLbBiCQ4OXrNmTV5e3rhx46A1N3/+fGj0QVmNpl97Ro0aFRcXByvi33//HTgBtYocYKbtb7a79LOVxUHhouTnQ0H35uqZpsM7by54t5fJo2atkz5DPMsLnFX8uRDZh+t9g83W8maHycc85p+XJTt/VB73kNRkhOrq6unTp5s8JJFIYGVq8hDMtrDJAZzDl3pMHoKWtrl8Bm0jk2WCAdktzbNrepk7amms44/vaq6fb567LtrkUZ1OV1NTY/KQSqVyczPduw8rBOfZH016TB6CVZBUajodwHD4vE0e2rG2DPYXzHwtApjBylARLAEj+3kkzbRv8LhrUJav+nnrjfkbelmIY6VlNmd1VMH5Zk0z6Ib8uq1qTIqVdGO9YTt+etDnqwpBN+PLN0vDe3sMGm1lApFN47z11Zod/yszV3l3PT7+b2Hi40Gxw62PL9o6y6D4knL/55X3jfYZM7Urj36UXWn57cvKHn3Fk2cF2xLfnilCBPhkZRGfj016KiSsdxccNv9u/Q3ZLfWo5ECredaI3RPU9n9WVZqvdBPz+gyWjJrqyJw4tpGbKb+UJYM9xH5homlL7JsA5eD0yF+/uHnjugL2qgqEuLuEJ/YSwAEBCnZxtpseieOw267jl+lnLXaaUQr7gkiy8z0YzuXxAaFrdzoGTN4sjExBq5gwfZE74Qt5GhWllOuUTTp1CwFvICBM+PjccGB/F6KD8hlQ1JN/HaqrKWtRNZM6HezixMj2s0vv+LWGEL3939Z9duf8XGMg/P3wsrB/0NwF20WmwB1dmubiw450nAcHgHg+gYJBI33C+zg+IoYk311g4sSJO3bs8PNjaX3F9pn1sGkI23mArXDyIcHJhwTb5dNqtXBQHLAVVstH6u0OY83LQlgtH8tzLuDkQ4TVN8fygg9wqQ8RTj4kOPmQ4ORDgu3ycVWH43CpDwlOPiQ4+ZCAZjMnn+NwqQ8JTj4kOPmQ4ORDgutxQYJLfUjweDxPT6Q9ppwN24eKGhsbAYthd9bg82H+BSyGkw8JTj4kOPmQ4ORDgu2GCyef43CpDwlOPiQ4+ZDg5EOCkw8JTj4kOPmQ4ORDgpMPCfbLx8ZVRenp6RkZGYYbo9dv6cFxPDs7G7AMNk5anzdvXs+ePXE9sNkLX6F85jZau7ewUb7AwMAJEya0D4HyTZkyBbAPli6ZePLJJyMjI40fw8LCUlJSAPtgqXxwgC05Odm4IObhhx/29vYG7IO9C3ZmzJhhKO9CQ0Mfe+wxwEoYrnlvXNXk5zQqlfTWa4bF3LDcp1orUP2yZ1zvR0i/ppx2J0TRL/qqlTKs/KZXkBP0OmZ4Ynn5jYLCgtCQkL59+1CUYSl16wpnGI0kWt/f/qJ2X6dfnt5pnbqbGz+kt3hgghgwB5PyffFmqVpJ8kWYYe+/1qX3+jXerd9h+P30H6Z/YwzU/9fLp3f1pJdUfzpJkfqaVx+ZbPPhhPMxgiANTqTar/GnX+Ef2RZuROSOazQUjwdS5oUFhDOzdydj8m19pTg0Rpz4xL3ZH9N2Lp1qOv/nrdSF4f5MKMiMfJ+tKIke5DVskg9wBTQq8MM7RfPWRwNkGKg6zh6QwRLGVbSDCN2Ap4/wx41VABkG5CvJV3h4OmuXYifhHy6S3VIBZBjoMmhp1gHcWTu0OwmMT2nUDOxty4B8pI6u6oBrQVEkwUCh301dfNKGIRMWBxPy0Zawq2VePQAZRlKf6+VdWjweA8mPiTYvibF6HydTkKTeHS4y3bTso/MtEymHCfl4rlbygdstaGSYkI+kW/3ApaC7IXgsqTooAICLlX60U26SJVUHvBlXy70YQ7+cgdRHjyG6XOoDHXawdBgmngEGgMuVfVA7jB2ZlyIARTKT+p6Y9n+ffb4ZOB8KMNNoY0A+mBHubaMtfdXyX3/bZ9cpGMbMLTMgH05ng3tZ9uXn2+2jku5jZ0mXAYVR9pZ9BEHs2v3tV19vhe9j+w96+j/PDxoU13pDfMGevd9v+eR9oVA4cGDcK8tXeUlpP0HFxYUZP+8+l5tdXV3ZMzJ68uSUKY/SPkrGjo+Hr+vfWb3lkw8yfjpi4w2wKPXxeJi9+ztu/XTjvn27VqW/s+LVtwICgl5+ZWFZWYnhUOaxwwpF87q3Ny5b+vrFi+e/+OJjQ/jmjzZkZ59KW/Ty22s/hNp98OG6v05nwfADv9Kvy5autF07Ggww0eHCWHepHTmhUd74w65vXkxbPiw+AX4cPnykUqmoq6+NiOgJP3p4iJ96stUhYNbJzL/zcg3vV65cC6OFBNOup4bExR84kHEm+2TC8JHAIegBTIolrQ7cvqqjpJj2/tGv34DWO+DzV6WvNx4dNDDO+N5L6q1Rq1s/UNSePTtPn8kqL291xhYSguI0nWJkiJEJs9nOqqO5mXYn5CYy68yo7cq3MxjsXlr+appWq3l2zoK4uHhPiefCtNkAARaVfTAjUPb0XojFtBsRmBNtP+Xaddpp5by5i0ePGgu1A7efgeMwZGgx0+qwq/eiV6++MIld+Puc4SPMRDBl/f67JefEjY20y8oA/9YpDCUlRfAfQIFipupgpOFM2dXqkEgkSRMmw5r3twMZuefPbty0PifndP/+Ay2cAi0VqPj3P2yXN8lhHQ1PgdVO9U16nFskEgUEBJ49+xe8FLAZuuRjor+PodRn56OE9gcswja8+9aSl+bm5Z1f9eZ6Q7VrjqCg4NdeXXP5St6UlHGvrlg8Z/b8Rx9NvXLl4n+eoU2/mTNmQXvwjTeWAZthaKSIiTkuX68uhZd4bFEkcB2yMqoLzzdb9sFmC0xUHa7WW8UgDMjncoO8QN9HyZrOesz10h/dVU+wpLPe5brqmYOJHhdAAdebpIGzxe5zxbKPtjgAAzA1y8DFCj82zbACgBkb9C6CMXTLjNS8Llf00eU1xZLUR5IuaDhjzHQ3M9Hfh7lg5WFYpIQMMz0uoLvCyAwrF2x2MAQD8gk9XG6KCxAIBSJ3BtaiMJB5pd4CNQMrTO4q8lqt0J2RKQLIJP0rWNmkAS5FbWVLzCAGtjRmQD6hBITHiHf+rwS4CHs3lQvd8JGP+gJkGFuQmnu0MftgfVCEe0QfCdFx3vDtNbhtbzrfBGjtdeh0lLptkjtwi1jHixgWXdeWqW4UKvyCRSkvhAAmYHI59IXMptxj9SoFqVUTHb7j9hLwO2UyKyjWtna8TX3jdW6Hg44XxDqI1frBeAWhEBO48SL7S8ZPZ8wjPduda0+aNOnbb7/lnGs7COfeGAlOPiRY7u2JS31IsFo+eiYFSfJ47F3pz3mLQYKTDwnO1RMSXOpDgpMPCU4+JLiyDwku9SHByYcEJx8SnHxIcPIhwcmHBCcfEpx8SHBmMxJc6kOCkw8JtnuLCQgIACyG1fIRBFFTUwNYDOerCAlOPiQ4+ZDg5EOCkw8JTj4k2C4ftF0Ai+FSHxKcfEiwXT7Y6QJYDJf6kODkQ4KTDwlOPiQ4+ZDg5EOCjauKFi5ceOLECeNWFziOkyQJP+bk5ACWwUYHs2lpaeHh4fhtgF7BiIgIwD7YKF+vXr1GjRrVPlvApJeYmAjYB3uda/fo0cP4Eb5PTU0F7IOl8oWFhY0fP97wHhZ88fHxBk/RbIO9zrWnT59u8O4OX6dNmwZYCZOGi/wWUXNDpVETHfbRxcwvWbaC6OERzx5RHRncd2BLTcDFGnnb9raVVAAABmNJREFU6cCRNeZ8HPCEPJ9AgX8YM66hAbrhcj1XmftHXV2NhtDR18H0rlIZ8Z7pDIz7LvEFmNRX2Od+z/gkb4CA4/L9ubv22hm5jqCEHnwPL3ffcE93L8aeqlPRqcj6CrmiTqVSqmESDo9xT37ewa0NHJGvvkz7w6ZymEN9Qr1C+iE9vXuOrFJZU1RPaHT3j/UdPtluB9d2y3dwe821XLkfFG4AAxt5sAQoYlV+jdRXMHO5fca5ffId+f5Wfk5T/7GRoCtScKpCKKD+vdKOX2eHfHs2VVaXqWPHsrHxxBTXT1YIeNTTb9qqoK123/7Pq6FR0rW1g/R+MIzCeV+uKrUxvk3yFV9sKbms6JfYNfNsJ6KGhahV5IGvb9oS2Sb5Dn5TFRTddSoKq/QdHVFwodmWmNblg9mWwjD/aCnoToi93L9aXWY1mnX5yq4qgmJYugeS84gaFqSQaWU1VqaIWJHvr1/rYUvHJ0wMWEmzomHpyuHn8w4DJyDw4B/aUW05jhX5oJUnkrhGU4xxfEKkdVVqy3GsyKeU63zDulepZ8Q/SqrTUQ3VlvKvpQ4rWQ1JEsA71Fk5V95U9/Nv75eU/63RqPr2TpiQOCswgLaNqm4Wbtg0Y9Hz244c++rilUwvaWDcoKTJSfMN2wnl/n3wwB+ftLTIY/uNThw5EzgTPg/POyEbk2q26LeU+grzmpy3nzVBEFu2vVBYcu7x5OUvLdghEft+uHVWbd0NQN80vRBr1761Q+6b+PYbJ2akpmdmfXvhEl3AVd0s2LH79fghk5e/+GN83D/27d8AnAnGx2urLeVfS/I11Wkwp/VGF5edr6kt+Vdqer8+I6SefsmTFok9vI+f2mmMMHjAuMEDx/P5gpio+/18wm5UXIWBJ0//6O0VnPTQbA8Paa/oocPjU4BTwSmV0tJAs6XMq9WSztuQuaT0Ao8n6B0db/gIx9KgTEUlucYI4aH9je/d3DxbVLRbwNr68uCgaGN4j7BY4GR0GksSWJJPIMSdN4beomomCC00O9oHSsRtPW6YqZSvVMr9/dpG4IRCd+BUSIBb3L7Nknw+wSLn7abuKfGDP37WzA6FF27N0S/Ms1pt2x7barUdji4dgaIkXiILxy3JFztEenyvs5aUhYX00WhavL2D/H1bRyDr6ivapz6T+HiHXL56HA5dGoS+nH8COBM4aBPUw5J8lp62QAJwPl5XiuaL1Ay9Y4b16z1i109vNciqmxWyrNO7P9jy9JlzP1s+a/CACbCl8dP+DbCbsqAo5+Tp3cCZUCQVN9ZSX4mVgUrYfy2ravaLZGB/8juZ9eS7p7L3fPPDitLyvAD/yPsHTxo9wsp4bt/ewx+ZuPDUmT3LXk+AVfDMJ9I3f/a8kzxe3MxvwPmYu8RSHCu9zX8fl5/IqI0d1y16+jpx7UR5YJgw5YVQC3GsFNX3jZbCqqe6QAa6HxqVzrJ2wJZZBn3vl+bnNAb3Mj0gCUvx19cmmTyk02mgZWfSI1VwQPSC5z4FzPH59iXFZRdMHtJq1QKBieJfKHB7/b/7gRkK/6r0DbJUaRiwaajo01eLPXw8wgaa3updLq81Ga7WtIjM2GU8Hl8sZnKAWKFsJHSmV4C0qBXuIlPNdgyDrR3TpzQSRWfL578TA6xhk3waJdi6omBgUhToHlz5s3TwaJ8Hk62PmtvUphV6gPgJgVeO2jr+5NIUZFX4hYhs0Q7YPlCZMFkal+h96Y8S0KWBScQ3hP/PxWE2xrdvlkFuZuOpn2tjRoSLPNi+W7sD5GeWewfwp71kxzxMu+e45P4pO/lLnYfULeqBYNBVqLzc0FDRGBkreWROkF0nOjhBbdsbJcomndjHPSretUWsvFLfWN3E42OPPhsaHGXdUumE4/P7ruUqjv1Y06LQ8fi4m0Tk6S/2DHB382T1jl0QtZJQ1KmabilUzWqdhhCIsAHDvUdOcXASAPKyGALs/6K6ukylbiEMk0oxgJHtrkmRoF3HnRk/0tQdrplab67VcTe8SYP53Xaw07vbH40xzVyewnAMjmAIhDy/UEHCJN/gaDeAAPOrilqa6YGM1g84BtpPdMb1npc6TXIGHZv8Bt+lJNkWRx9IO36iH8Vtd0+GY8YLdvIHxcMB0e4KrYHA3Y3H7DI0trt6Yjld0P64m3DyIcHJhwQnHxKcfEhw8iHx/wAAAP//tQ6kdgAAAAZJREFUAwAs01CgvyQOPQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(conversation_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f77cb444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_proper_memory(\n",
    "    message: str,\n",
    "    session_id: str = \"default\"\n",
    ") -> dict:\n",
    "    \"\"\"Chat with proper memory management using trim_messages\"\"\"\n",
    "    \n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": f\"conv-{session_id}\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=message)]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = conversation_agent.invoke(initial_state, config=config)\n",
    "        \n",
    "        final_message = result[\"messages\"][-1]\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": final_message.content,\n",
    "            \"total_messages\": len(result[\"messages\"])\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat failed: {e}\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "757c4314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 1 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROPER CONVERSATION BUFFER WITH trim_messages\n",
      "============================================================\n",
      "\n",
      "--- Turn 1 ---\n",
      "User: Hi, my name is Alice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Trimmed to 2 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing with 3 messages\n",
      "INFO:__main__:Trimmed to 4 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing with 5 messages\n",
      "INFO:__main__:Trimmed to 6 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Hello Alice! It's nice to meet you. This is our first conversation, so I don't have any prior knowledge about you. How can I assist you today?\n",
      "(Total messages in state: 2)\n",
      "\n",
      "--- Turn 2 ---\n",
      "User: What's my name?\n",
      "Agent: You mentioned your name earlier as \"Alice\". Is there something specific related to your name that brings you here today?\n",
      "(Total messages in state: 4)\n",
      "\n",
      "--- Turn 3 ---\n",
      "User: I work as a software engineer at Google\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing with 7 messages\n",
      "INFO:__main__:Trimmed to 8 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You're working as a software engineer at Google, which is one of the largest and most influential tech companies in the world. That's impressive! What aspects of software engineering do you focus on, or are there any specific projects or areas that interest you?\n",
      "(Total messages in state: 6)\n",
      "\n",
      "--- Turn 4 ---\n",
      "User: What do I do for work?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 9 messages\n",
      "INFO:__main__:Trimmed to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You mentioned earlier that you're a software engineer at Google. To recap, you're a professional who designs, develops, and tests software programs, often working on complex technical problems to create innovative solutions for companies like Google. Does that sound about right?\n",
      "(Total messages in state: 8)\n",
      "\n",
      "--- Turn 5 ---\n",
      "User: I'm interested in AI and machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 11 messages\n",
      "INFO:__main__:Trimmed to 12 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: That's a fascinating area of focus! As a software engineer at Google, you likely have opportunities to work with various AI and ML technologies, such as natural language processing, computer vision, or predictive analytics.\n",
      "\n",
      "Are you looking to dive deeper into any specific aspect of AI/ML, like building conversational interfaces, developing recommender systems, or exploring the latest advancements in deep learning?\n",
      "(Total messages in state: 10)\n",
      "\n",
      "--- Turn 6 ---\n",
      "User: What are my interests?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 13 messages\n",
      "INFO:__main__:Trimmed to 14 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You mentioned earlier that you're interested in Artificial Intelligence (AI) and Machine Learning (ML). That's a broad and exciting field with many potential applications. You also work as a software engineer at Google, which suggests that you have a strong technical background.\n",
      "\n",
      "To recap, your interests include:\n",
      "\n",
      "1. AI\n",
      "2. ML\n",
      "\n",
      "Is there anything specific within these areas that captures your attention?\n",
      "(Total messages in state: 12)\n",
      "\n",
      "--- Turn 7 ---\n",
      "User: I live in San Francisco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 15 messages\n",
      "INFO:__main__:Trimmed to 16 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing with 17 messages\n",
      "INFO:__main__:Trimmed to 18 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You mentioned earlier that you work as a software engineer at Google, and now I see that you also live in San Francisco! That's great - living in the heart of Silicon Valley can be very convenient for tech professionals like yourself.\n",
      "\n",
      "San Francisco is known for its vibrant tech community, with many startups and established companies like Google calling it home. Does your location play a role in your work or interests at all?\n",
      "(Total messages in state: 14)\n",
      "\n",
      "--- Turn 8 ---\n",
      "User: Where do I live?\n",
      "Agent: You mentioned earlier that you live in San Francisco!\n",
      "(Total messages in state: 16)\n",
      "\n",
      "--- Turn 9 ---\n",
      "User: Do you remember my name\n",
      "Agent: Your name is Alice, and we also discussed that you're a software engineer working at Google, with an interest in Artificial Intelligence (AI) and Machine Learning (ML). And, of course, you live in San Francisco. Is there anything else I should remember?\n",
      "(Total messages in state: 18)\n"
     ]
    }
   ],
   "source": [
    "session = \"user-1234\"\n",
    "    \n",
    "conversations = [\n",
    "    \"Hi, my name is Alice\",\n",
    "    \"What's my name?\",\n",
    "    \"I work as a software engineer at Google\",\n",
    "    \"What do I do for work?\",\n",
    "    \"I'm interested in AI and machine learning\",\n",
    "    \"What are my interests?\",\n",
    "    \"I live in San Francisco\",\n",
    "    \"Where do I live?\",\n",
    "    \"Do you remember my name\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROPER CONVERSATION BUFFER WITH trim_messages\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, msg in enumerate(conversations, 1):\n",
    "    print(f\"\\n--- Turn {i} ---\")\n",
    "    print(f\"User: {msg}\")\n",
    "    \n",
    "    result = chat_with_proper_memory(msg, session_id=session)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(f\"Agent: {result['response']}\")\n",
    "        print(f\"(Total messages in state: {result['total_messages']})\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca621e",
   "metadata": {},
   "source": [
    "### Summarization pattern for short term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "996cd25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "691f427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b663dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"\"\"Summarize this conversation concisely, preserving key information:\n",
    "     {conversation}\n",
    "     provide a 2-3 sentence summary: \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "114949f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_chain = summarize_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0c1567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_summary(state: SummarizationState) -> dict:\n",
    "    \"\"\"Chat using summary + recent messages\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        m for m in state[\"messages\"]\n",
    "        if isinstance(m, (HumanMessage, AIMessage, SystemMessage))\n",
    "    ]\n",
    "    \n",
    "    # Build context with summary\n",
    "    context_messages = []\n",
    "    \n",
    "    if state.get(\"summary\"):\n",
    "        context_messages.append(\n",
    "            SystemMessage(content=f\"Conversation summary: {state['summary']}\")\n",
    "        )\n",
    "    \n",
    "    context_messages.extend(messages)\n",
    "    \n",
    "    # Invoke LLM\n",
    "    response = llm.invoke(context_messages)\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9912ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_trim(state: SummarizationState) -> dict:\n",
    "    \"\"\"Summarize old messages and trim\"\"\"\n",
    "\n",
    "    messages = list(state[\"messages\"])\n",
    "\n",
    "    if len(messages) < 10:\n",
    "        return {}\n",
    "    \n",
    "    logger.info(\"creating summary and trimming messages...\")\n",
    "\n",
    "    # summarize last 4 messages and remove those later\n",
    "    old_messages = messages[:-4]\n",
    "\n",
    "    conversation_text = \"\\n\".join([\n",
    "        f\"{'User' if isinstance(m, HumanMessage) else 'Assistant'}: {m.content}\"\n",
    "        for m in old_messages\n",
    "    ])\n",
    "\n",
    "    summary_response = summarize_chain.invoke({\"conversation\": conversation_text})\n",
    "    new_summary = summary_response.content\n",
    "\n",
    "    if state.get(\"summary\"):\n",
    "        combined_summary = f\"{state['summary']}\\n\\nRecent: {new_summary}\"\n",
    "    else:\n",
    "        combined_summary = new_summary\n",
    "\n",
    "    \n",
    "    logger.info(f\"Created summary: {new_summary[:100]}...\")\n",
    "    \n",
    "    # Remove old messages\n",
    "    messages_to_remove = [RemoveMessage(id=m.id) for m in old_messages]\n",
    "    \n",
    "    return {\n",
    "        \"summary\": combined_summary,\n",
    "        \"messages\": messages_to_remove\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cbdfddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(SummarizationState)\n",
    "workflow.add_node(\"chat\", chat_with_summary)\n",
    "workflow.add_node(\"summarize\", summarize_and_trim)\n",
    "\n",
    "workflow.set_entry_point(\"chat\")\n",
    "workflow.add_edge(\"chat\", \"summarize\")\n",
    "workflow.add_edge(\"summarize\", END)\n",
    "\n",
    "conn = sqlite3.connect(\"summarization_memory.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "summarization_agent = workflow.compile(\n",
    "    checkpointer=checkpointer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cdcb06b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(summarization_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a83ba01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_summarization(message: str, session_id: str = \"default\") -> dict:\n",
    "    \"\"\"Chat with automatic summarization\"\"\"\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": f\"sum-{session_id}\"}}\n",
    "    \n",
    "    result = summarization_agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=message)], \"summary\": \"\"},\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"response\": result[\"messages\"][-1].content,\n",
    "        \"summary\": result.get(\"summary\", \"\"),\n",
    "        \"message_count\": len(result[\"messages\"])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b044da80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARIZATION PATTERN\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: Here is a concise summary of our conversation:\n",
      "\n",
      "The conversation started with the user providing som...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After message 1:\n",
      "  Messages in buffer: 34\n",
      "  Has summary: True\n",
      "  Summary: Here is a concise summary of our conversation:\n",
      "\n",
      "The conversation started with the user providing som...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: Here is a concise summary of the conversation:\n",
      "\n",
      "The user sent a series of messages to keep track of ...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: Here is a concise summary of the conversation:\n",
      "\n",
      "The user sent a series of messages with content to r...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: This conversation started with the user providing content to remember in each message, and the assis...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: The conversation started with a series of messages where the assistant kept track of \"content to rem...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: The conversation involved a repeated process of sending and receiving messages with \"content to reme...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After message 6:\n",
      "  Messages in buffer: 1088\n",
      "  Has summary: True\n",
      "  Summary: The conversation involved a repeated process of sending and receiving messages with \"content to reme...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: Here is a 2-3 sentence summary:\n",
      "\n",
      "This conversation started with the user providing a series of messa...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: It looks like there was an error in the input. The text seems to be incomplete and doesn't contain a...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: I can't fulfill this request....\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: I'm happy to help, but it seems like there was an error in your request. You provided a prompt with ...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: I'm happy to help, but it seems there was an error in your request. You didn't provide any text for ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After message 11:\n",
      "  Messages in buffer: 34816\n",
      "  Has summary: True\n",
      "  Summary: I'm happy to help, but it seems there was an error in your request. You didn't provide any text for ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: I can help you with that. However, I don't see any text to summarize. Could you please provide the t...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: There is no text to summarize. Please provide the text you would like me to summarize, and I will be...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: I don't see any text to summarize. Please provide the text you would like me to summarize, and I wil...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: I'm happy to help, but it seems there was an error in your request. You asked me to provide a 2-3 se...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARIZATION PATTERN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "session = \"sum-test\"\n",
    "\n",
    "# Long conversation\n",
    "for i in range(15):\n",
    "    msg = f\"This is message {i+1} with some content to remember\"\n",
    "    result = chat_with_summarization(msg, session_id=session)\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        print(f\"\\nAfter message {i+1}:\")\n",
    "        print(f\"  Messages in buffer: {result['message_count']}\")\n",
    "        print(f\"  Has summary: {bool(result['summary'])}\")\n",
    "        if result['summary']:\n",
    "            print(f\"  Summary: {result['summary'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d492aa",
   "metadata": {},
   "source": [
    "### Long term memory with vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7911c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from typing import TypedDict, Annotated, Sequence, List\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d1642ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorMemoryState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add]\n",
    "    retrieved_memories: List[str]\n",
    "    user_id: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "041b3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.7)\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3372eca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX512 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX512 support.\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created new vector store\n"
     ]
    }
   ],
   "source": [
    "# Create vector store (initialize once)\n",
    "try:\n",
    "    vector_store = FAISS.load_local(\n",
    "        \"./faiss_memory\",\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    logger.info(\"Loaded existing vector store\")\n",
    "except:\n",
    "    # Create new vector store\n",
    "    initial_docs = [\n",
    "        Document(\n",
    "            page_content=\"System initialized\",\n",
    "            metadata={\"timestamp\": datetime.now().isoformat(), \"type\": \"system\"}\n",
    "        )\n",
    "    ]\n",
    "    vector_store = FAISS.from_documents(initial_docs, embeddings)\n",
    "    vector_store.save_local(\"./faiss_memory\")\n",
    "    logger.info(\"Created new vector store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7455256",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SYSTEM_PROMPT = \"\"\"You are a helpful assistant with long-term memory.\n",
    "\n",
    "Retrieved relevant memories:\n",
    "{memories}\n",
    "\n",
    "Use these memories to provide context-aware responses. Reference past conversations naturally.\"\"\"\n",
    "\n",
    "memory_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", MEMORY_SYSTEM_PROMPT),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "memory_chain = memory_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78debb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_memories(state: VectorMemoryState) -> dict:\n",
    "    \"\"\"Retrieve relevant memories from vector store\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get current message\n",
    "        current_message = state[\"messages\"][-1].content\n",
    "        \n",
    "        logger.info(f\"Retrieving memories for: {current_message[:50]}...\")\n",
    "        \n",
    "        # Search vector store\n",
    "        docs = vector_store.similarity_search(\n",
    "            current_message,\n",
    "            k=3,  # Top 3 most relevant memories\n",
    "            filter={\"user_id\": state[\"user_id\"]} if state.get(\"user_id\") else None\n",
    "        )\n",
    "        \n",
    "        # Format memories\n",
    "        memories = []\n",
    "        for doc in docs:\n",
    "            timestamp = doc.metadata.get(\"timestamp\", \"unknown\")\n",
    "            memory_type = doc.metadata.get(\"type\", \"conversation\")\n",
    "            memories.append(f\"[{timestamp}] ({memory_type}): {doc.page_content}\")\n",
    "        \n",
    "        logger.info(f\"Retrieved {len(memories)} memories\")\n",
    "        \n",
    "        return {\n",
    "            \"retrieved_memories\": memories\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Memory retrieval error: {e}\")\n",
    "        return {\n",
    "            \"retrieved_memories\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c96642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_with_memory(state: VectorMemoryState) -> dict:\n",
    "    \"\"\"Generate response using retrieved memories\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Format memories\n",
    "        memories_text = \"\\n\".join(state[\"retrieved_memories\"]) if state[\"retrieved_memories\"] else \"No relevant memories found.\"\n",
    "        \n",
    "        # Invoke LLM\n",
    "        response = memory_chain.invoke({\n",
    "            \"memories\": memories_text,\n",
    "            \"messages\": state[\"messages\"]\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [response]\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Response generation error: {e}\")\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {str(e)}\")]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6bf54b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_memory(state: VectorMemoryState) -> dict:\n",
    "    \"\"\"Store important information in long-term memory\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get last few messages\n",
    "        recent_messages = state[\"messages\"][-2:]  # Last user message and AI response\n",
    "        \n",
    "        # Extract user message and AI response\n",
    "        user_msg = None\n",
    "        ai_msg = None\n",
    "        \n",
    "        for msg in recent_messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                user_msg = msg.content\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                ai_msg = msg.content\n",
    "        \n",
    "        if user_msg and ai_msg:\n",
    "            # Check if this is important to remember\n",
    "            # Simple heuristic: if user shares personal info\n",
    "            important_keywords = [\"my name is\", \"i am\", \"i work\", \"i like\", \"i live\", \"i have\"]\n",
    "            \n",
    "            should_store = any(keyword in user_msg.lower() for keyword in important_keywords)\n",
    "            \n",
    "            if should_store:\n",
    "                # Create memory document\n",
    "                memory_content = f\"User: {user_msg}\\nAssistant: {ai_msg}\"\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=memory_content,\n",
    "                    metadata={\n",
    "                        \"timestamp\": datetime.now().isoformat(),\n",
    "                        \"user_id\": state.get(\"user_id\", \"unknown\"),\n",
    "                        \"type\": \"conversation\"\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Add to vector store\n",
    "                vector_store.add_documents([doc])\n",
    "                vector_store.save_local(\"./faiss_memory\")\n",
    "                \n",
    "                logger.info(\"Stored new memory\")\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Memory storage error: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "828616ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(VectorMemoryState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve_memories)\n",
    "workflow.add_node(\"respond\", respond_with_memory)\n",
    "workflow.add_node(\"store\", store_memory)\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"respond\")\n",
    "workflow.add_edge(\"respond\", \"store\")\n",
    "workflow.add_edge(\"store\", END)\n",
    "\n",
    "# Compile\n",
    "conn = sqlite3.connect(\"vector_memory_agent.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "vector_memory_agent = workflow.compile(checkpointer=checkpointer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "caaadaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_vector_memory(\n",
    "    message: str,\n",
    "    user_id: str = \"default\"\n",
    ") -> dict:\n",
    "    \"\"\"Chat with long-term vector memory\"\"\"\n",
    "    \n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": f\"vector-mem-{user_id}\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "        \"retrieved_memories\": [],\n",
    "        \"user_id\": user_id\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = vector_memory_agent.invoke(initial_state, config=config)\n",
    "        \n",
    "        final_message = result[\"messages\"][-1]\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": final_message.content,\n",
    "            \"memories_retrieved\": len(result[\"retrieved_memories\"])\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat failed: {e}\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0b63e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGoAAAGwCAIAAADOkWc9AAAQAElEQVR4nOydB3wURfvHZ/dKei+kkUACgYSSAAGCICAhFAXpHaQoUpQiTRBU6otSfLGgyN/CC1JEUGlKkSIEpHeQkkYSSAipl3pt9//sbXK5hOtDyJLM98Pn2NuZ2d375Zmy0x4xy7KIYC1iRMCAyIcFkQ8LIh8WRD4siHxY4MqXdKsk4UpBzhOFUsGo5CxiES1mGRWFaJYWUWolSyGKohHXOuKCEKNCiIJ/LGIgBMJYVnNQdgYhWsIySu6AEiGGYSm2/FjNahJw14GLlydHDMsl5dAEcQlYquxGPBAZwpiKZ5Y6iEQ06+AqbRDm0KyDI8KAsq7dd/mo7MaZ3MJ8FQgjtRGJJEhqS8OVWDVb9ugiRNMUd8CynHzcD2BFEk5Q7nfSFFJzB5SIYlXcA1A0xTLcgVhKqxSMRjI4gzTCI/hLMGqENJfhPvjIFKARTOcnsNw5eAaaUZUJxkVmy67DY2MnlpeqVQpWIVfDZW0dRA2bOb4y1BNZjsXyXT6af/GvbIZB3gG2Ud09AsNs0ItMUTZ7cl/mw/vFKiXToLljrzfqWZTcMvm2rEgpkqnCo106D/BAtYt/zxWeOfAEiouJy4PNT2WBfN/MTagXZDfwXT9UezmxK+vWP3kd+3hFvuJiTnxz5Vs/K77bUN+waAdUB/h6TvyoBQ1dPEQmY5ol3/rZCZNWhIhtUd3h2/mJUTGebWKdjUejkSk2zEvsNrxendIOmPRJ8LnDT/KfqIxHMyHf/5Y+8KpvE9YWq3H0ghLdy3P7mgfG4xiT7+KRvKJC1aBp/qhO0jrGBZqEv3yeZiSOUfmO5rRo74rqMENmBGWmlBqJYFC+y8dkjIp9eVBta99ZhIMLZeco2v3FQ0MRDMp3/VSud6Ader7ExsY+fPjQ0lQJCQl9+vRB1UNkF/cnDw0aoEH54O2iXexzNb309PTc3FxkObdv30bVRutuLvCGmnJXv4L6e1zirxTBe341vc9CS3P79u379+9/8OBBw4YNo6Ojp0yZcuXKlcmTJ0Nov379unTpsnbtWrCpXbt2Xbhw4dGjR8HBwf379x88eDB/hZiYmLfeeuvYsWOQasyYMVu2bIGTUVFR77333qhRo9CzxsZedDMuL7CJz9NB+uVLvFUksaFQ9bBjx44ffvhh5syZHTt2PHHixPr16x0cHMaPH79u3To4uWfPHn9/rq4HBUG4hQsXQh9KcnLyp59+6uvrC0kgSCKR/Pbbb+3atQMR27RpAxEOHz4Mfw9UPTi5iHMyFXqD9MtXkK20ta+untTLly+Hh4fzpdWAAQPatm1bXFz8dLSVK1cWFRX5+XGv2GBZe/fuPXPmDC8f6OXi4jJnzhz0XHD1kqTFl+gN0q+RXK6WSKvL+iIiIr788sulS5e2atWqc+fOAQEBeqNBHgc7PX36NORx/gxvlTzwB0DPCxsHkVKh1hukXz5GzdC06fc56xg5ciTk1r///nvJkiVisRhq2+nTp3t5eVV6AIaZMWOGQqF49913wfScnJzefPNN3QhSqRQ9LyiuW1u/MemXj+uPLWZQ9QB/mAEaEhMTz58/v3HjxsLCwv/+97+6ce7cuXPr1q2vv/4aCjj+TEFBgbe3N6oJSgrAmCyRz9FZkp9VjKoHKOPDwsJCQkKCNYAuUA9UiZOXlwefWr0SNUASVBPIcpQSG/2dV/pzaGC4g1JeXdZ38ODBuXPnnjx5Mj8/Py4uDtofUBrC+QYNGsDnkSNHbt68CbJCvoYWiUwmg2p39erV0L6BhqHeCwYGBmZlZUElri0lny0FuUo3L4neIP3yNYt2hG7AvAz95SUmixYtAnVmzZoFzbdly5ZBKw9aJ3Ae6pC+fftu2LABKhYfH5/ly5ffuHGjW7du0Jp75513oNEHsmqbfrp06tQpMjISKuJDhw6haqC0SB0W5aQ3yGB36f8tTPQKsO0/pTZ3zZvDv+cLj+3MeGdNI72hBqvXplHO6cklqM5z7lCWq6fBWt5g2/jlAZ43zuRdOyGL6Kq/wzojI2P48OF6gxwdHaEy1RsE2RZeOVD1sEmD3iDN8K/+fAZtI71lAk9BjnLSfxoZCjU21nF8Z1b89YKJyxvqDVWpVJmZmXqDSktLbW319+5DhVB97Y8CDXqDoApydtZvB3Ae/t56g7avSmXU7KgFgcgAJoaKvluUFNjUvsdoywaPawcp90r3bUwzVOrxmHi1eGt5w/tXCkvzq6sRI2QO/N/DTv1MZBTTb2axI+v9uCIJ1TF+XPygflOniJdNDFSaNc6bk6HYvjr1nbU10+h//nwzL6HLQO/waCeTMc2dZZB8s3jf948iO7tCjYxqLyl3Sv748VFgE4dXJ/iYE9+SKUIs+vaDRImU7j3Wxze4Fg6bb1uVmv9E8XJ/r+Ydnc1MYvEEtT++z0i+UwTjT41bOnWqFfOsrp6UQV+8LFvp4WczbHaARWmtnB755w8ZaQklilIGelXtncT2zmIpdO6LuOmR2ji0mGJU5V/5/p7yb9D/w0J/KMMfQ+8e4qZQVv7UPc9doPJJ6ILj5p5yB9oz3ExTbg6rpjosS6uZUAltZohMc1NUuSCRWKSQq4vz1SVFakWJmhZRHr7SwVMDkARZipXy8RTlMOf+ynmSWlosUyq4KaE0oytf+ePquSutmUiriQsdkdyPfPpTo5eaVYsorrOIn5WrjQDfOaE1fxVWM/uWQhUX5E9WRNZ8rZAPriih7OxFbvUkLTq6BYRaPyKGJd9zoGfPntu2bfPwEGgpIfSZ9fBqCO95SKgQ+bAg8mEhdPmUSiUMiiOhImj5YLgSaUbmkFARtHwCz7mIyIeJoB9O4AUfItaHCZEPCyIfFkQ+LIQuH6k6rIdYHxZEPiyIfFhAs5nIZz3E+rAg8mFB5MOCyIcF6XHBglgfFiKRyMnJ9DSnGkToQ0X5+flIwAg7a4jFkH+RgCHyYUHkw4LIhwWRDwuhN1yIfNZDrA8LIh8WRD4siHxYEPmwIPJhQeTDgsiHBZEPC+HLJ8RVRUuWLNm7dy//YPBJaaBp+sKFC0hgCHHS+pQpUxo0aEBrgNde+AT5DG20VrMIUT5vb+/Y2FjdMyBfv379kPAQ6JKJUaNGBQUFab/6+/v3798fCQ+BygcDbK+//rp2QUyPHj1cXYW4g7RwF+yMGDGCL+/8/PwGDhyIBIllNe/pPTkF+UqlnFtVzC3sZrkFx7Rm0bZ2AXO5J5wyrzia1d4s0qz/5u6luZuIRuryrWEobg269nE0qcuD0tPT7t1P8PP1a9y4MSpfEV0WUXMXfnE54nzslHks4h4G6TxGxWr1sq88mmiVziBuR1nazlnSroeHndlb9Jsr3x/fP35wr1gs4jRSynnvQmUrwilOR87BEi8f72ypXD5uiTm/7FujHMULQNEaD0+o6jGnA02x5fLRYlal5K5btpSfKvP8VPH05Uv1tUFat1Jl7qC0cXS15x8e6d6XQyShaBFSlDJuXjYj5plV0ZslX9ye7NvnZH3fbuDoVl37wQqK379Ks7FDQ2eZVtC0fMd/zkq4VTRsdhCqS+z7No2m2eFz6huPZrrquH+tsFl7N1TH6DspIPexwmQ0E/IpCpFKoW7eydxtYWoTIgl9Zn+O8TgmugwKC9RMXfV/zKjZ0kITu7ea6nGh1Gxd3HyOQ61m1SoTpkNcfGJB5MPCpHw0qqNFH7/tFm7mZVCdaCnrgX8/Mh6HZF4sTMpH1VXj03hPpknNay3sU30KT2NSvrraaDYPM6yP6GcYM+Srw4Wfyd9Oyj5jUKaKLpMdVnRNVb27f90RE9sO1SBc/7iJH29SvurtcFmydP4ff+7RGxQe1nzM6LeQsKnhzHv37u22bTvoDQoLaw7/kLB59gOVkOkGDekZd/oEZL0v16+BMzk52ctXLBw+sk//gd1XrPwwNbXMIdMrMVHpGY9Wr1nWt19X+NpvQMzu3dtnvDcRzssKZLqZV6VSfbvxi/FvDn2tb+f3F0w/ezaOPz9txpvz3n9X9+4LFs6c+u44I0nMhxax8M9EHPSskUqlxcVFe/fuWjB/6YB+Q9Vq9XuzJ129dum9mR/88N3Pbq7uU98Z+/BRGsQ8+Mdp+Jw758N9e04gjefJ/X/81qhRk9Wr1tvb2ete84svV+3avW1A/2Hbtu7r0jnm4yXz/j55FM6/0iX20uXzRUVFfLTS0tKLF89279bLSBLzYdQU/DMex6R8FtccFEXBzxg+fGz3mF4BAYE3blxNSUn+YMGy9u1ecnf3mDJ5prOL6+7d2/QmdHZ2mfbOnKg27XVXQcvl8kOH948cMe71voNcnF1e7d0vpluvzVv+D4K6dOnOMMypuGN8TDB5+Nq1a6yRJM+W6ppl0LRJM/7gxs2rYFatW7Xlv4JGkRFtrl2/rDdVk1A9nifv3ftXoVC0jaooIuEKiYnx+bJ8Dw9POD4Vd5w/f/r0iTat28EfyVASkBWZDUWx+O0+KyterQ/JwsICpVIJxZluqKurm/FUusAVkKaYq3I+NycbLAts7av1a8DeRSLRP2dPTZ82z0iSUnmpjY25W6yzLGXyjavaa14wEDs7uxXLK7mgFNEiC67gyXn/nD1rob9/pVFXb2/OpQbIB8XcmX9OgvRczu0SaySJg70DMhuwPpN79pqWj8Jr+IWEhJaUlMBP9fcrG7R/lP7Q1cWCgeMA/0DeZFpFlplwbm4OjO7b23PVCxggZNjz58/I5aUdX+rCnzSUxKKNJcD6GFPDZKarDkRjvXbAb2vX7qU1a5Y9fpyRn5/3+55fJk8Zc/DgXgiCX+jl5Q115ZWrF43MYQZFxo2dBAU/1EJQokEFOmfe1HWff6KNABXI9euXL106B5ZoZpJnhRllH/Zbx8oV6/bu2710+YLbt2/Urx/UvXvvgQPL/OONGjnhx00bzl84s32bMdfYw4e9AVa8bcemy5fPOzg4NgtvOXv2Im0oZNjP/vsf+GOA9ZmZ5FlhYo5LToZi66cp4xY3QnWPzcsSQiOdYkcb8zVGOusNQosoSoTbWV93e5sZNcuqcTvraYqus/ZnGtPjvFVmsBJ0IWUfFqTsw8J0K5ytq0Nt8MYmwh8mp+rqUBu8samxh8lJ0WcM02UfRcbJDWNO2UcM0CCk4YIFabhgYVI+KV1X53FIbUTwz3gcE92l7j7cS29Blun1NbUPtZrxa2RnPI7pkTYHF/E/f2ajOsadc/nQbG7cyt54NNPyvbEw8ElKycN7gt4Q5Jlz8a+cl16rZzKauet5v52f6OQsqR/m6OQuVqn1JNGul6WemlBJc/PzDdze8OzLKkFVv7JIu7SXrdK4p4xO6TQcCuamKEapdwuy0uWj5wY5eZkeDrRgNfmudQ9zM5UqlVql1CcfVclZtqEHrhJqRCOKqvS+/dRlK0SrehGq7Ec9/SSaYM3Kc30/WkQjsUTk6Cp+fXx9R9OWx19M2C2TXr16bd26lTjXthLi3hgLIh8WAvf2RKwPztWb2AAAEABJREFUC0HLB9UawzAikQXziZ4zxFsMFkQ+LIirJyyI9WFB5MOCyIcFKfuwINaHBZEPCyIfFkQ+LIh8WBD5sCDyYUHkw4I0m7Eg1ocFkQ8LoXuL8fLyQgJG0PKp1erMzEwkYIivIiyIfFgQ+bAg8mFB5MOCyIeF0OWDtgsSMMT6sCDyYSF0+aDTBQkYYn1YEPmwIPJhQeTDgsiHBZEPCyGuKpo2bVpcXBxVvo6dpmmGYeDrpUuXkMAQooPZGTNmBAQE0OUgjYKBgYFIeAhRvkaNGnXq1Ek3W4DpdenSBQkPgbo3Hj16dP36Fbu2wvHgwYOR8BCofP7+/jExMfwxFHxRUVG8p2ihIVzn2sOHD+e9u8PnsGHDkCAxt+GS8q+8SKZUa3bi1fpk5qDKFyZXqcCpsg10NOuSNU6ktUGsjiMWtnzRcuXraE7a9Ojw1vHSEy2aNC/J9LqZKdMmrbgdq2eXKK2j7So3rPTATx/rYGcvDYmwRWZguuFy4PuMtPvFoBurRmXyVaih+6hlx/x/5Q/Gli8LN/jwlc+UXbnMzXnFAvXylf7c81IG/ljlx+VX0RvHyNJ2LWIpDbfx8LYdNscfGcWEfCd+zoq/WfTSa/Xqh5n116g1FGazR3c8BHHGLDTmINqYfHu+zsh9ohg0U4gNrufD4c0ZBTnycR8HGYpgrOp4lFT06oS6qx3Q4w0feYn65ulCQxEMynf+oEwkEdnVRa/alXBwlty7LDMUarDmlWXL2TrrGFoHSsQUFxks3wzKp2ZU6rq1cZB+uE1rDJtRXd0ezWw0TTWDRRyRzwQSCWVkC0iD8lEURTaOBFRqKMgMln0GzZJlBb4903OC2/nW8E4ehjMv925E9IOeWmMyGM682o+6jVptbNt02lgIUY/blY424qHcoPWRso9HpWZIw8V6uLKPsdz6SMOFh2FYI6WYkYYL2fGag3M1RBl8aRPuWEd1sO7zT8a/OdSiJJzxsVaUfSTnauDKPmR52UeyLg/Xa8c+l5r348XzRCJRvXq+O37evGTxqs4vd7t16/r/Nm+8c+eWi6tbh+iXx77xtoODg+aZ2N2/bj90aH9q2oOgwIZRUdETxk+BtDt/+Wnb9k1zZi36bN1/8vJy/fwC3hj9Vo8er/HXP336b7jag5QkFxfXRo2azJj2fr16nJPUJUvnQzXXPab3J6sWl5QUh4e3mPz2DN6veXFx8YqVi65cudCwYaN+fa0ZaKdFIiOvHQZ1taLilUgkiUnx8G/Fss9atmiV9jB1zryppfLSr778cdmSNYmJ99+b9TY/Y+rXX3f8tPWHwYNG7ti2v2/fQQf++B0Uh/MikbioqPDosYNbt+z5/bejMd16giK8L/OLl859tHguSLlzxx8ff/jJ48fp674oc9kpFotv3b5+5K8/Nnyz5c8DcTZSm5WffswHrVm7LC0tZc3qb+ABkpITzp6LQxYCg/Rqw73GxuSztPiDJBkZj5Z8vOqllzq7urr99defErEEnjswsEGDBsFzZn94P/5u3OkTEPPa9ctNmoT37NkHovV5bcD6rza1b9eRvwjoO3DAcDs7O2cn53FjJznYOxw9dgjO//DjN2DOoDiYXrNmLadOmXX2bNydu7f5VCXFxXPnfOTn6w9SxnTrBYqD3WVlPTl+4siI4WPDw5q7u3tMenu6jY3l44VqzT8DGJQPVLei9IOcaGtb9oi3bl1r2rQZ/Fr+q4+PL2TG6zeuwHHz5hGXLp1btXrpwUP78mX5/n4BjRqFai8SGhrGH8DfA5KkpCTBMRgvXE0bh3fDDcUC/7V+YAPeMy/g6OgEnwUFsvT0h9wjBQVXpGoSjiyFQlVH3XUw3GymrWk2S3VcVxcWFoB1VHGrnZvDOV8AI7K3dzh95u9PVy0Be+naNXbSxOmenmULx3X9X9vY2kJ2BuRyua7t8GIVFxfxX2l9fpzzZXlcTLsKrwd2tiYcSDwNP05vKNTwOy+D+87r7uHZokXk+HGTdU+6OHPGCL8W8iz8S05OvHz5/KbNG0Gj/5Q74C4qKuJrGEBeWurm6s5bdGlpifY6RRrhPNw9jTwAfy8ofLVntHKbj9GaozqbzSHBjTMzMyJatm4VGcX/AyGgHIQgqHOTkhLgAMrEgQOHDxo4Ij7+rjbhlasX+AOwuJTU5IYNQ8BCm4SGQT2ujcMfB4c0NvIAPj5+8Hnz5jX+q1KphPoHWQhjdLzRcNUBQRSW+Q0ePAoK0K++XltaWgpl+bcbv5jw1jColyEI6laoRs+cOQkFH9QAp+KONW8WUfZANA31ckpKslqthuoCFISqAM4P6D8Mqp3du7fLCmRXrl78+pvPWrdq27hREyMP4OXlDYXspk0b4O5wneUrFlpRHnHNPsP6Gc68iKnwiGEVUHV+/93PO3b8b9KU0SAHFPxz53wY2rgp4nzWL/pq/ZqFH86CY6gTIRcPGTyaTwW/cOiQ0bPmTM7OzoL6d/68xfXrc3MkoMnyJCvz51+2wN8DmntRbaInvvWuyWdYMH/punUr3548CkyvV8++r/bux1f95iMSU4zhHheDc1wO/5SRcL149MJg9BzZ/esOMKujR84jwfDrl8mg0thF+qe5kO5SEzBGZ1oYlA9eoUSC3j7qOUFpijFDGKw6oORWP/e1oIMGDhdUzuWA9q+IDJNbC1fzGg4lZR8Whss+MXTVEP2gO4e2prtUrWLUapJ7oQeIseadlxR95mCkt5n4NTaNEevDfeetHUgkFGtF2celYYn5QT8Na03Zx/UYEOMzhVHrI5jCoHwSGOaRksyLbG3FDGv5JA03LxuSeQGlgrF3Mth3YlC+VjHOahX7OKUueiXXpVimatPFoHtbY2MdjVo6Hd/xCNVh9nyZ5uohDWwmNRTBxILUW2cKTu3NCm3t0rqzu8jiQb4XmFtn8m+fzfMKsOk70cdINNPLoc/+kX/znzxFKbwDI8s6YVirp2mZSmk0XNNetT45IKIoiR0dEGLfe4IJH9sWbIOjLkFqfSscnnbqrm+ZeNlja6ni7103gm7ygQMHbty40cvTExm4WpUrGwrSfRj+1k8n1B5L4Wea55XVghlWkHmfv6dXubzA3h6aUEiYCH1qOHFvjAWRDwsiHxZEPush3mKwIPJhQeTDgsiHhVKpJPJZD7E+LIh8WBD5sCA+KrEg1ocFkQ8LknmxINaHBZEPCyIfFqTsw4JYHxZEPixAu3r16iEBI3Tre/z4MRIwxFcRFkQ+LAQtH7RaiI9K6yHWhwWRDwsiHxbEuTYWxPqwIPJhQeTDgsiHBZEPCyIfFkQ+LIh8WBDn2tYwceLECxcu8PtpalxSUvzBlStXkMAQ4qbrU6ZM8ff35z1ri0Qi/oD45zWX1q1bR0ZG6mYLePONiIhAwkOgW/6PGTPGz89P+xWOR40ahYSHQOVr2rRphw4deANkGCY8PDwsLAwJD0E71+a9u3t7e48cORIJEuHKFxwcDAYIphcaGtqqVSskSJ5BwyX5VsmJXzJLilVqVeWLVVm0zeosQa68mFu7/ptiGZai9UfSc01Dq8IrRypv+pRBIajMJbZ0WBvnTv3dER64zea8TPWfm9PrN3Js2cnVuZ4U+oa1y8SrLNrWPc8+dcbQAe8DXnuMDCwi114TPbVOnWYRo6OeCCGFHN2Iy7l1TmbrQEfFuiIMsKzvRlzhmQNZI+c3QC8mO9c+8K5vYrMH42CVfWcPZoW3c0MvLP2nBaXdK0IYoynWy5eTzqoUTGQ3F/TCIpUiia3o+K4sZC3Wl32PkgtrgQ9QkYjNz5Uja7FePkbFqFQvvHwKOauUW/8riJNFLIh8WNR1+bgdWjF2Kazr8rF4e4ySzIsFjnwUqvM7Y+PIVxtc0JKyDwtS9mFBUcT6MMB0gl3na14ay5EzhnzQVVgLfAIwCNWM9UEfeJ33CSCIoaKkpIThI/ugFxBBlH13791GLybPVb6CwoIfN204dzYuNy+nSWh49+69X3u1P5zZvOU7CH0lJmrqlPeGDB5VXFz82br/XL16saBA1iAouHfvfv37DUGch+P4NycOX7li3ZrPlru6un23cbtKpfr+h6/PnovLzMxo3jxyQL+h0dGdLHokVGMNF8tvu2rVkidPHs+cuSAosOHve3b+d91KUGf8uMkKheL4icM7tu3no83/YDrosmzpWj9f//0Hfvv8i0+bNAkPa9qMX5i/+afvhg0dA2LB8Rdfrvrz4N5p787t0qX76dMnPl4y74MFy7p0jrHgmfAaLhhln+W3vXb9cufOMW2jor296709cdr6rzZ5eHhViXP23OkbN67Onf1hmMYx96iR41u0iPzf5o2Id5eOECQHC4VQuVx+6PD+kSPGvd53kIuzy6u9+8V067V5y/+h54j18llR64IQO3/56ZsN686cOalUKpuEhvn4+FaJk5QUb2tr27BhiPZMaOOwu3dv637lD+7d+xfMtm1UB21QZEQbyOD5snxkPjSiMUzI+sxrRZvv/XmL9+7ddez4IRDR0cFxwIBhb4yZWGW3guzsLNvKHsTt7e1LSoq1X7XuzwsLC+Bz2ow3q9wlNycbjBGZCWPCB6pxnmvV4ezkPHrUBMiPN29eOxV3fMtP3zs6Og0dMlo3joODg64PcqRxQ+75VB4HPDSu4GfPWujvX1/3vLe3JcPeVI29dVAW3Rjq04OH9kEJBXkTcjH8i4+/e+/+nSrRoEYuLS29H39X6zj7339vNtDJy1oC/ANtNJbYKjKKP5Obm8OyLFgrMh8W660Dr+qw5MY0TUMNsHjp+2B6OTnZhw8fuB9/p4WmAg0ICIQ8Gxd3IjX1Qbt2L/n5BXz22Yo7d29DNGiXgHzDhox5+oIg07ixk6CugKoGCsG/Tx6dM2/qus8/Qc+R55d5weiWLl795frVfGkFlcPkSTN793odjqPbdwIdP/x4ztg33h439u3lS9du+Hbd1HfGSqXS4ODGy5auAVPVe83hw94ICQndtmPT5cvnHRwcm4W3nD17EXqOWD9F6Pqp/JO/PRn7cSP0IrPtk0R3H+mQGVbOO8cp+1AtgKqphgt68Se4IE1nfQ01XIgHSzz5UG0AWv81k3lrh/FBHqqhzMtNukZ1G6y3DqoWjHXU2FBR7aDGhopIxUtqXkzIBDUsiHxYYPQ205RIJNwVhWYikdA4PsStl8/Z05Z+8WsP+AX2jtY7ELXefIKaSCF1wpUi9CJTWsJE9/JC1oKV+0JaOF05av2Kphpn74Y0F0+Js/XqYa/nvXw07+KRvOg+3g1bWDLCUNMUZCoObc1wdhMNmuGPMHgGy6GPbMtKvC6DZiDDIrXuMi29/pepyg1GzJNP38XUeTgtklLQzecVYDt4uh/C45ltg5N0RZ6bV8owFas7uVdi7tqszleW/9SsHtcVmtI3Z4GT6rdf9/Ts1cPe3q5MuadichfiQvgrspUSU3rkoyna1lEa3u7Z5BUh7iKkS2xs7M6dO93cBLpqWEkUIUUAAAg6SURBVOjNZoG77CDujbEg8mEhdPnUajWRz0pAO5FIhAQM8c+LBfFVhAWRDwsiHxbEzR0WxPqwIPJhQeTDgsiHBak6sCDWhwWRDwsiHxZCl4+UfdZDrA8LIh8WRD4saJr28sKYQlH9CN36srIEPYeG+CrCgsiHBZEPCyIfFkQ+LIh8WBD5sCDyYUHkw4LIhwWRDwsiHxZEPiyIfFgQ+bAg8mEhxGUxkyZNSkxMRJq5zXl5eXZ2dgzDKJXKixcvIoEhxPXMw4cPB6PLzc2VyWTQXy+Xy0E7X19fJDyEKN8rr7wSGhqqmy3guHHjxkh4CHQ1/fjx4z08PLRfYcBoxIgRSHgIVL7o6Ojw8HCtc+2QkJC2bdsi4SHcvRy0Bujm5jZkyBAkSIQrX0RERGRkJNQhQUFBXbt2RYLkGTRcHtwsvvx3bk6GUiFXcxejKEbFlvvL5hYwazxm895UWe2yaO7e3MpmzRdtKEVVeH/WnCl3jU1RNLfTYznlkSh+73dKN4n2Itoblf1UqmyDfxt7kbObuElr55adnREeWPL99lV6RkqxWsWKxLTEXmLraGNjJ2ZpFn4nv+pbK1WlVfU0J6jm3uU+ybV6sPyelBUSsBRLM1T5n6IclttzVKO95iRVkVizgr1cvsr3FYkoVo2UcpWiRKUoVqiVUKiyHj7SPm/Wd7R2tbWV8u3bmJ5yt0gsEbvXd/EKxv0b1hSyxyVPknLlxQpXT8nI9wOR5Vgj37fzk+CzQWs/G6dasodT/NmHiiJFt2E+Tds6WpTQMvlyMhTbVqV41HfxbeqOahcFmfIH1x5FdHZ7ub+H+akskC//iWrLJw/CX2lAC3qJLRa3/kruPNCrRUdziyNz5cvLYLauTmrWvQGq7fx74kHTNk6vDDVrYpy57b6tqxPrN/NGdYCwrkG3zuU/TlKYE9ks+X5cnGznZOPs+yJts4SDd0O3XetTzYlpWr7bZwuLC1TB7XH3K3qB8A5xlUhEezZkmIxpWr5Tv2c6eTmgOoZ/c++0+4Umo5mQ7+F9uVLBBEYItNQrLMqd82H7qzf+Qs8aB3cbSkQf3PzYeDQT8p05kGXrYP3mii80zh6Oqf8WG49jQr7M1BIHdztUJ/Ft6lpaamKgysRbF8Mgn5DqesGQFWTv+3Ndcup1haK0SePo7l0meHsFwfn0xwlrvxo5fdIPx07+7+a/f7s4e0e2iH019h1+S5wr1w8fPPptSYksvOnLXTqOQtWGSCqCHogLh3Pb9jDYo2DM+u5dKOC8YVXPojIYRdvww9SE5MuD+s6f/e42Rwf3LzZOyMpOgyCxiLvlL3tWtmrZ85OP40YOXvL36a3XbnEFXPrj+G27Popq9er8mbujIl/bc2Atqk7EUtHjB3IjEYzJl5GmoEXVtTluUsrVzKzkEYOXNA3t4Ozk0bfXdAd711P/7NBGiGjWLaJ5jFgsCWnY2sPNP+0h55LszLndri4+sV3ftLd3bhTcpn1Uf1SdwM+X5SiNRDCWeRWlaixHPkZJfnBNJJI0Di5zsQbZBGRKTL6ijRDgF6Y9trV1KinlfAJm5aT61AvWnq/vH46qE4o24Q3RmHyatNU1iF5SWqhWK6HZoXvS0aGilOH6l5+iuFjm6VHhE1Aqrd5qjeL26jQWwZh8Du6S6puB4OToAT9+wqhKhRdtytghzyqVpdqvcnn17hrNsKxEYkwiY2GBoQ6XjuSg6sHfN1ShKHF1refpXuYjLTvnoa716cXN1ff2nVPQy84LfftuHKpOGDXr5mVjJIKxv7ZvQymYb1GOsarHahqHtG3auMMvv6/IzcsoLMo7fW7X5xvGnb+8z3iqiGbd4U3j9wNroZ8tPvHSmXO7UHWiVqgbtzb2wmqi3WfrIMp+IHNwr5ZVoRNGf/bPhV9/2rnoQeoNL8+g1hG9Xu4wzHiSJo3b9+k57Z/zv879KBqq4FFDlqz/blI1uQ7JTy+Ggi+wqbHi1UR36ZFtTxKuFzbtYs0wyotO/Nl0GxtmzAfGfruJojp2pJdaqS4pUKO6h7xI3r6niTcu00NlHr62qdfTQzvqd4IJpfhHK2P1BqlUCmjZUfo8wPt4Bb/79rN0Iv79lllJKdf0BimVcolET/Evldh+NO8AMkDqjWyJlA5tY2Lgzayxjq/nJDZs42fnqv/1LSf3kd7zpaWFtrb6b0/TYleXZ9kJJpNlqdT6u9eLimUO9nqHfih3N4NzBm8fTe71hl9wSxPtSrMGasOjne9ceNS0a5DeUHe3mu+Idnb2NBRkxePFn33kVk9qUjtk5lhH18Gejq7ihHPpqA6Qfi9PJVeOmFvfnMjmvtKOXhDIqpT3z9RyBTPuy3JScyd/EmxmfMtmGez47GGJjG3YXojTjPFJu5ktyyyYujrE/CQWz3HZsiKlIE8Z3M7f1lHQu0tZyr1TaYxaPflTc+2Ox5opQsd/ybr1T57UXtKwrT+Oly6BkHTpcXFuiU+Q7aDpFjvesX5+37ZPU3My5fDq7uRh7xXsauv0QhmjGj1OyM3PLFKUquydxP0m+Xv4WjNbDHd26b6N6Q8TS1Ryhpu4SVMiiYhl2MrX1MxSLJsCWsnhkGb+ZOVoqDy8atqy0HJfPRVnyiepVr1O+cV17shPumS4yAzDisUiT3+b2BH1XLysn/L0zFYVJVwtTr1fXFKkkpcwalXFNFp+Um35ZNOKT00QyzLlv7tcDZqmGH7uqWYuLaTl+wB5B+zloazmQOf6Wi35Obzlk3bLQstOUlIb2t5ZXM/frnkny+bxGULovooETi2ZHlpTEPmwIPJhQeTDgsiHBZEPi/8HAAD//0A+OFAAAAAGSURBVAMAq8l7gJMFe7gAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(vector_memory_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24a48463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Retrieving memories for: Hi, my name is Alice and I'm a software engineer...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Retrieved 2 memories\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VECTOR MEMORY DEMO - First Conversation\n",
      "============================================================\n",
      "\n",
      "User: Hi, my name is Alice and I'm a software engineer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Stored new memory\n",
      "INFO:__main__:Retrieving memories for: I work primarily with Python and I love building A...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Retrieved 3 memories\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Nice to meet you, Alice! It's great to have you here. As we previously discussed, I recall that you're primarily working with Python and have a passion for building AI applications.\n",
      "\n",
      "I'd love to catch up on what's new in your world. Are you still exploring the latest developments in machine learning and deep learning? Have you come across any interesting projects or challenges recently?\n",
      "\n",
      "If not, we can always start from scratch and discuss the latest trends and technologies in the field. I'm here to help and provide guidance whenever you need it.\n",
      "\n",
      "What's been on your mind lately, Alice?\n",
      "\n",
      "User: I work primarily with Python and I love building AI applications\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Stored new memory\n",
      "INFO:__main__:Retrieving memories for: My favorite hobby is rock climbing...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Retrieved 3 memories\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: A software engineer with a passion for AI! That's fascinating, Alice. As we previously discussed, I recall our conversation when you mentioned your interest in building AI applications.\n",
      "\n",
      "Python is an excellent language to work with, especially for machine learning and deep learning tasks. Have you had any experiences with popular Python libraries like TensorFlow, Keras, or scikit-learn? Or perhaps you're interested in exploring newer frameworks like PyTorch or Hugging Face?\n",
      "\n",
      "What specific area of AI are you most interested in? Natural Language Processing (NLP), Computer Vision, Reinforcement Learning, or maybe something else? I'm here to help and would love to discuss your projects or provide guidance on any challenges you're facing.\n",
      "\n",
      "By the way, what kind of AI applications have you been working on recently? Are there any specific use cases or industries that interest you?\n",
      "\n",
      "User: My favorite hobby is rock climbing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Retrieving memories for: What's my name?...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Retrieved 3 memories\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Rock climbing! That's a fantastic hobby, Alice. It requires physical strength, mental focus, and problem-solving skills - all qualities that are also essential for building AI applications.\n",
      "\n",
      "I'm curious, how do you think the skills you've developed through rock climbing might translate to your work in AI? For example, the ability to approach complex problems from different angles or the need to stay calm under pressure could be valuable assets in machine learning and deep learning.\n",
      "\n",
      "Do you find that your experience with rock climbing has influenced any of your projects or approaches to AI applications?\n",
      "\n",
      "============================================================\n",
      "VECTOR MEMORY DEMO - Later Conversation\n",
      "============================================================\n",
      "\n",
      "User: What's my name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Retrieving memories for: What do I do for work?...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Retrieved 3 memories\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Your name is Alice, a software engineer with a passion for building AI applications. I've been chatting with you about various topics, including your love for Python and machine learning, as well as your hobby of rock climbing. Is there anything else you'd like to talk about or ask about?\n",
      "(Retrieved 3 relevant memories)\n",
      "\n",
      "User: What do I do for work?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Retrieving memories for: What are my hobbies?...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Retrieved 3 memories\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You're a software engineer, Alice! You primarily work with Python and have a passion for building AI applications. You've been exploring the latest developments in machine learning and deep learning, and we chatted about some of the projects you've been working on and your interests in different areas of AI. How can I assist you further today?\n",
      "(Retrieved 3 relevant memories)\n",
      "\n",
      "User: What are my hobbies?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You enjoy rock climbing as a hobby, Alice! It's great to have something outside of work that brings you joy and challenges you in new ways. Do you have a favorite climbing spot or a particularly memorable climb that stands out to you?\n",
      "(Retrieved 3 relevant memories)\n"
     ]
    }
   ],
   "source": [
    "user = \"alice-123\"\n",
    "    \n",
    "# First conversation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VECTOR MEMORY DEMO - First Conversation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "messages_1 = [\n",
    "    \"Hi, my name is Alice and I'm a software engineer\",\n",
    "    \"I work primarily with Python and I love building AI applications\",\n",
    "    \"My favorite hobby is rock climbing\"\n",
    "]\n",
    "\n",
    "for msg in messages_1:\n",
    "    print(f\"\\nUser: {msg}\")\n",
    "    result = chat_with_vector_memory(msg, user_id=user)\n",
    "    if result[\"success\"]:\n",
    "        print(f\"Agent: {result['response']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VECTOR MEMORY DEMO - Later Conversation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Later conversation - test memory recall\n",
    "messages_2 = [\n",
    "    \"What's my name?\",\n",
    "    \"What do I do for work?\",\n",
    "    \"What are my hobbies?\"\n",
    "]\n",
    "\n",
    "for msg in messages_2:\n",
    "    print(f\"\\nUser: {msg}\")\n",
    "    result = chat_with_vector_memory(msg, user_id=user)\n",
    "    if result[\"success\"]:\n",
    "        print(f\"Agent: {result['response']}\")\n",
    "        print(f\"(Retrieved {result['memories_retrieved']} relevant memories)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16fcfff",
   "metadata": {},
   "source": [
    "### Enterprise based memory architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "39aade14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence, List, Dict, Optional, Literal\n",
    "from operator import add\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass, asdict\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4849c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef87dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data models\n",
    "@dataclass\n",
    "class Memory:\n",
    "    \"\"\"structured memory object\"\"\"\n",
    "    content: str\n",
    "    memory_type: Literal[\"semantic\", \"episodic\", \"procedural\"]\n",
    "    importance: float # 0 to 1\n",
    "    timestamp: str\n",
    "    user_id: str\n",
    "    metadata: Dict\n",
    "\n",
    "    def to_document(self) -> Document:\n",
    "        \"\"\"convert to langchain document\"\"\"\n",
    "        return Document(\n",
    "            page_content=self.content,\n",
    "            metadata={\n",
    "                \"memory_type\": self.memory_type,\n",
    "                \"importance\": self.importance,\n",
    "                \"timestamp\": self.timestamp,\n",
    "                \"user_id\": self.user_id,\n",
    "                **self.metadata\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_document(cls, doc: Document) -> \"Memory\":\n",
    "        \"\"\"create from langchain document\"\"\"\n",
    "        metadata = doc.metadata.copy()\n",
    "        return cls(\n",
    "            content=doc.page_content,\n",
    "            memory_type=metadata.pop(\"memory_type\", \"semantic\"),\n",
    "            importance=metadata.pop(\"importance\", 0.5),\n",
    "            timestamp=metadata.pop(\"timestamp\", datetime.now().isoformat()),\n",
    "            user_id=metadata.pop(\"user_id\", \"unknown\"),\n",
    "            metadata=metadata\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d65fdaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state\n",
    "class ProductionMemoryState(TypedDict):\n",
    "    \"\"\"complete production memory state\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add]\n",
    "\n",
    "    # Memory retrieval\n",
    "    retrieved_memories: List[Memory]\n",
    "    memory_query: str\n",
    "\n",
    "    # User context\n",
    "    user_id: str\n",
    "    session_id: str\n",
    "    \n",
    "    # Memory management\n",
    "    buffer_size: int\n",
    "    compression_threshold: int\n",
    "    conversation_summary: str\n",
    "    \n",
    "    # Metadata\n",
    "    turn_count: Annotated[int, add]\n",
    "    total_memories_stored: Annotated[int, add]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b327d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryManager:\n",
    "    \"\"\"Centralized memory management system\"\"\"\n",
    "    def __init__(self, storage_path: str=\"production_memory\"):\n",
    "        self.storage_path = Path(storage_path)\n",
    "        self.storage_path.mkdir(exist_ok=True)\n",
    "\n",
    "        self.embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "        \n",
    "        self.stores = {}\n",
    "        for memory_type in [\"semantic\", \"episodic\", \"procedural\"]:\n",
    "            store_path = self.storage_path / memory_type\n",
    "            try:\n",
    "                self.stores[memory_type] = FAISS.load_local(\n",
    "                    str(store_path),\n",
    "                    self.embeddings,\n",
    "                    allow_dangerous_deserialization=True\n",
    "                )\n",
    "                logger.info(f\"Loaded {memory_type} memory store\")\n",
    "            except:\n",
    "                # Create new store\n",
    "                init_doc = [Document(\n",
    "                    page_content=f\"{memory_type} memory initialized\",\n",
    "                    metadata={\n",
    "                        \"timestamp\": datetime.now().isoformat(),\n",
    "                        \"memory_type\": memory_type\n",
    "                    }\n",
    "                )]\n",
    "                self.stores[memory_type] = FAISS.from_documents(init_doc, self.embeddings)\n",
    "                self.stores[memory_type].save_local(str(store_path))\n",
    "                logger.info(f\"Created {memory_type} memory store\")\n",
    "\n",
    "    def store_memory(self, memory: Memory) -> None:\n",
    "        \"\"\"Store memory in appropriate vector store\"\"\"\n",
    "        try:\n",
    "            store = self.stores[memory.memory_type]\n",
    "            doc = memory.to_document()\n",
    "            store.add_documents([doc])\n",
    "            \n",
    "            # Save to disk\n",
    "            store_path = self.storage_path / memory.memory_type\n",
    "            store.save_local(str(store_path))\n",
    "            \n",
    "            logger.info(f\"Stored {memory.memory_type} memory for user {memory.user_id}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error storing memory: {e}\")\n",
    "\n",
    "    def retrieve_memories(\n",
    "        self,\n",
    "        query: str,\n",
    "        user_id: str,\n",
    "        memory_types: List[str] = None,\n",
    "        k: int = 5,\n",
    "        importance_threshold: float = 0.3\n",
    "    ) -> List[Memory]:\n",
    "        \"\"\"Retrieve relevant memories\"\"\"\n",
    "        \n",
    "        if memory_types is None:\n",
    "            memory_types = [\"semantic\", \"episodic\", \"procedural\"]\n",
    "        \n",
    "        all_memories = []\n",
    "        \n",
    "        for memory_type in memory_types:\n",
    "            if memory_type not in self.stores:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                store = self.stores[memory_type]\n",
    "                \n",
    "                # Search with filters\n",
    "                docs = store.similarity_search(\n",
    "                    query,\n",
    "                    k=k,\n",
    "                    filter={\"user_id\": user_id}\n",
    "                )\n",
    "                \n",
    "                # Convert to Memory objects\n",
    "                memories = [Memory.from_document(doc) for doc in docs]\n",
    "                \n",
    "                # Filter by importance\n",
    "                memories = [m for m in memories if m.importance >= importance_threshold]\n",
    "                \n",
    "                all_memories.extend(memories)\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error retrieving {memory_type} memories: {e}\")\n",
    "        \n",
    "        # Sort by importance and recency\n",
    "        now = datetime.now()\n",
    "        \n",
    "        def score_memory(mem: Memory) -> float:\n",
    "            \"\"\"Calculate memory relevance score\"\"\"\n",
    "            try:\n",
    "                timestamp = datetime.fromisoformat(mem.timestamp)\n",
    "                days_old = (now - timestamp).days\n",
    "                recency_factor = 1.0 / (1.0 + days_old * 0.1)\n",
    "                \n",
    "                return mem.importance * 0.6 + recency_factor * 0.4\n",
    "            except:\n",
    "                return mem.importance\n",
    "        \n",
    "        all_memories.sort(key=score_memory, reverse=True)\n",
    "        \n",
    "        return all_memories[:k]\n",
    "    \n",
    "    def cleanup_old_memories(\n",
    "        self,\n",
    "        user_id: str,\n",
    "        days_old: int = 90,\n",
    "        importance_threshold: float = 0.5\n",
    "    ):\n",
    "        \"\"\"Remove old, unimportant memories\"\"\"\n",
    "        # This is a simplified version\n",
    "        # In production, you'd implement proper cleanup logic\n",
    "        logger.info(f\"Cleanup old memories for user {user_id}\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7edfc46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MemoryAnalyzer:\n",
    "    \"\"\"Analyze conversations and extract memories\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOllama(model=\"llama3.2\", temperature=0.3)\n",
    "    \n",
    "    def analyze_conversation(\n",
    "        self,\n",
    "        user_message: str,\n",
    "        ai_message: str\n",
    "    ) -> List[Memory]:\n",
    "        \"\"\"Analyze conversation and extract memories\"\"\"\n",
    "        \n",
    "        analysis_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"human\", \"\"\"Analyze this conversation and extract important information to remember.\n",
    "\n",
    "User: {user_message}\n",
    "Assistant: {ai_message}\n",
    "\n",
    "Extract:\n",
    "1. Facts (semantic memory) - general knowledge, definitions\n",
    "2. Personal info (episodic memory) - user's experiences, preferences\n",
    "3. Procedures (procedural memory) - how to do things, workflows\n",
    "\n",
    "For each item, provide:\n",
    "- content: what to remember\n",
    "- type: semantic, episodic, or procedural\n",
    "- importance: 0.0 to 1.0\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\"memories\": [{{\"content\": \"...\", \"type\": \"...\", \"importance\": 0.8}}]}}\n",
    "\n",
    "Analysis:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            chain = analysis_prompt | self.llm\n",
    "            response = chain.invoke({\n",
    "                \"user_message\": user_message,\n",
    "                \"ai_message\": ai_message\n",
    "            })\n",
    "            \n",
    "            # Parse JSON\n",
    "            response_text = response.content.strip()\n",
    "            \n",
    "            # Remove markdown code blocks if present\n",
    "            if \"```json\" in response_text:\n",
    "                response_text = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "            elif \"```\" in response_text:\n",
    "                response_text = response_text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "            \n",
    "            data = json.loads(response_text)\n",
    "            \n",
    "            memories = []\n",
    "            for item in data.get(\"memories\", []):\n",
    "                memory = Memory(\n",
    "                    content=item[\"content\"],\n",
    "                    memory_type=item[\"type\"],\n",
    "                    importance=item[\"importance\"],\n",
    "                    timestamp=datetime.now().isoformat(),\n",
    "                    user_id=\"\",  # Will be set by caller\n",
    "                    metadata={}\n",
    "                )\n",
    "                memories.append(memory)\n",
    "            \n",
    "            logger.info(f\"Extracted {len(memories)} memories\")\n",
    "            return memories\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Memory analysis error: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bde6f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummarizer:\n",
    "    \"\"\"Summarize long conversations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOllama(model=\"llama3.2\", temperature=0.3)\n",
    "    \n",
    "    def summarize(self, messages: Sequence[BaseMessage]) -> str:\n",
    "        \"\"\"Create conversation summary\"\"\"\n",
    "        \n",
    "        if len(messages) < 5:\n",
    "            return \"\"\n",
    "        \n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"human\", \"\"\"Summarize this conversation, preserving key facts and context:\n",
    "\n",
    "{conversation}\n",
    "\n",
    "Provide a concise 2-3 sentence summary.\n",
    "\n",
    "Summary:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            # Format conversation\n",
    "            conv_text = \"\\n\".join([\n",
    "                f\"{'User' if isinstance(m, HumanMessage) else 'Assistant'}: {m.content}\"\n",
    "                for m in messages\n",
    "            ])\n",
    "            \n",
    "            chain = summary_prompt | self.llm\n",
    "            response = chain.invoke({\"conversation\": conv_text})\n",
    "            \n",
    "            return response.content.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Summarization error: {e}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7e5831f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created semantic memory store\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created episodic memory store\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created procedural memory store\n"
     ]
    }
   ],
   "source": [
    "memory_manager = MemoryManager()\n",
    "memory_analyzer = MemoryAnalyzer()\n",
    "conversation_summarizer = ConversationSummarizer()\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "beb4890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_memories(state: ProductionMemoryState) -> dict:\n",
    "    \"\"\"Retrieve relevant memories for current query\"\"\"\n",
    "    \n",
    "    try:\n",
    "        current_message = state[\"messages\"][-1].content\n",
    "        user_id = state[\"user_id\"]\n",
    "        \n",
    "        logger.info(f\"Retrieving memories for user {user_id}\")\n",
    "        \n",
    "        # Retrieve memories\n",
    "        memories = memory_manager.retrieve_memories(\n",
    "            query=current_message,\n",
    "            user_id=user_id,\n",
    "            k=5\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"retrieved_memories\": memories,\n",
    "            \"memory_query\": current_message\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Memory retrieval error: {e}\")\n",
    "        return {\n",
    "            \"retrieved_memories\": [],\n",
    "            \"memory_query\": \"\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ba82cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_conversation_buffer(state: ProductionMemoryState) -> dict:\n",
    "    \"\"\"Manage conversation buffer with summarization\"\"\"\n",
    "    \n",
    "    try:\n",
    "        messages = state[\"messages\"]\n",
    "        buffer_size = state.get(\"buffer_size\", 10)\n",
    "        threshold = state.get(\"compression_threshold\", 20)\n",
    "        \n",
    "        # Check if summarization needed\n",
    "        if len(messages) >= threshold:\n",
    "            logger.info(\"Conversation threshold reached - creating summary\")\n",
    "            \n",
    "            # Summarize old messages\n",
    "            old_messages = messages[:-buffer_size]\n",
    "            summary = conversation_summarizer.summarize(old_messages)\n",
    "            \n",
    "            # Keep only recent messages\n",
    "            recent_messages = messages[-buffer_size:]\n",
    "            \n",
    "            return {\n",
    "                \"conversation_summary\": summary,\n",
    "                \"messages\": recent_messages\n",
    "            }\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Buffer management error: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d62bee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(state: ProductionMemoryState) -> dict:\n",
    "    \"\"\"Generate response using memories\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Format memories\n",
    "        memories_text = []\n",
    "        \n",
    "        if state.get(\"conversation_summary\"):\n",
    "            memories_text.append(f\"Previous conversation: {state['conversation_summary']}\")\n",
    "        \n",
    "        for memory in state.get(\"retrieved_memories\", []):\n",
    "            mem_type = memory.memory_type\n",
    "            importance = memory.importance\n",
    "            content = memory.content\n",
    "            memories_text.append(f\"[{mem_type}, importance: {importance:.1f}] {content}\")\n",
    "        \n",
    "        memories_str = \"\\n\".join(memories_text) if memories_text else \"No prior context.\"\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a helpful assistant with memory of past conversations.\n",
    "\n",
    "Relevant memories and context:\n",
    "{memories}\n",
    "\n",
    "Use this context naturally to provide informed, personalized responses.\"\"\"),\n",
    "            (\"placeholder\", \"{messages}\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | llm\n",
    "        response = chain.invoke({\n",
    "            \"memories\": memories_str,\n",
    "            \"messages\": state[\"messages\"]\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [response],\n",
    "            \"turn_count\": 1\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Response generation error: {e}\")\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {str(e)}\")]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "39f06255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_memories(state: ProductionMemoryState) -> dict:\n",
    "    \"\"\"Extract important information and store as memories\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get last exchange\n",
    "        recent_messages = state[\"messages\"][-2:]\n",
    "        \n",
    "        user_msg = None\n",
    "        ai_msg = None\n",
    "        \n",
    "        for msg in recent_messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                user_msg = msg.content\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                ai_msg = msg.content\n",
    "        \n",
    "        if not (user_msg and ai_msg):\n",
    "            return {}\n",
    "        \n",
    "        # Analyze and extract memories\n",
    "        memories = memory_analyzer.analyze_conversation(user_msg, ai_msg)\n",
    "        \n",
    "        # Set user_id and store\n",
    "        user_id = state[\"user_id\"]\n",
    "        stored_count = 0\n",
    "        \n",
    "        for memory in memories:\n",
    "            memory.user_id = user_id\n",
    "            memory_manager.store_memory(memory)\n",
    "            stored_count += 1\n",
    "        \n",
    "        logger.info(f\"Stored {stored_count} new memories\")\n",
    "        \n",
    "        return {\n",
    "            \"total_memories_stored\": stored_count\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Memory storage error: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a211a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(ProductionMemoryState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"retrieve_memories\", retrieve_relevant_memories)\n",
    "workflow.add_node(\"manage_buffer\", manage_conversation_buffer)\n",
    "workflow.add_node(\"generate\", generate_response)\n",
    "workflow.add_node(\"store_memories\", extract_and_store_memories)\n",
    "\n",
    "# Define flow\n",
    "workflow.set_entry_point(\"retrieve_memories\")\n",
    "workflow.add_edge(\"retrieve_memories\", \"manage_buffer\")\n",
    "workflow.add_edge(\"manage_buffer\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"store_memories\")\n",
    "workflow.add_edge(\"store_memories\", END)\n",
    "\n",
    "# Compile with checkpointing\n",
    "checkpointer = SqliteSaver.from_conn_string(\"./production_memory_agent.db\")\n",
    "production_memory_agent = workflow.compile(checkpointer=checkpointer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
