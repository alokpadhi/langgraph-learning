{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32d9dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alokpadhi/langgraph-learning/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "from operator import add\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, trim_messages\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25833389",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefa069",
   "metadata": {},
   "source": [
    "### Short term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9febf3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State with conversation buffer\n",
    "class ConversationState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add]\n",
    "    buffer_size: int  # Max messages to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83ffd493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27d3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful assistant with memory of the conversation.\n",
    "Reference previous messages when relevant to provide continuity.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7afbf47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36f994f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40ecb418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state: ConversationState) -> dict:\n",
    "    \"\"\"Chat node with automatic buffer management\"\"\"\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Processing with {len(state['messages'])} messages in buffer\")\n",
    "        \n",
    "        # Trim messages if buffer is too large\n",
    "        buffer_size = state.get(\"buffer_size\", 10)\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "        if len(messages) > buffer_size:\n",
    "            # Keep most recent messages\n",
    "            messages = messages[-buffer_size:]\n",
    "            logger.info(f\"Trimmed buffer to {buffer_size} messages\")\n",
    "        \n",
    "        # Invoke LLM\n",
    "        response = chain.invoke({\"messages\": messages})\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [response]\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat error: {e}\")\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {str(e)}\")]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb30c354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x74f42f04e950>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(ConversationState)\n",
    "workflow.add_node(\"chat\", chat_node)\n",
    "workflow.set_entry_point(\"chat\")\n",
    "workflow.add_edge(\"chat\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9783ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72049795",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"conversation_memory.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "conversation_agent = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bede91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dec52f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(conversation_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8fa5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_memory(\n",
    "    message: str,\n",
    "    session_id: str = \"default\",\n",
    "    buffer_size: int = 10\n",
    ") -> dict:\n",
    "    \"\"\"Chat with conversation buffer\"\"\"\n",
    "    \n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": f\"conv-{session_id}\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "        \"buffer_size\": buffer_size\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = conversation_agent.invoke(initial_state, config=config)\n",
    "        \n",
    "        final_message = result[\"messages\"][-1]\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": final_message.content,\n",
    "            \"total_messages\": len(result[\"messages\"])\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat failed: {e}\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6335875b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 13 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CONVERSATION BUFFER DEMO\n",
      "============================================================\n",
      "\n",
      "--- Turn 1 ---\n",
      "User: Hi, my name is Alice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 15 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Nice to meet you properly, Alice! I remember our conversation earlier. You've been open with me about your job as a software engineer and your interest in AI and machine learning. Is there anything on your mind that you'd like to chat about or ask for advice on? I'm here to listen and help if I can.\n",
      "(Buffer size: 14 messages)\n",
      "\n",
      "--- Turn 2 ---\n",
      "User: What's my name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 17 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Your name is Alice! We've already established that, but it's nice to remind ourselves. You're having a bit of fun with our conversation, aren't you? Don't worry, I won't hold it against you!\n",
      "(Buffer size: 16 messages)\n",
      "\n",
      "--- Turn 3 ---\n",
      "User: I work as a software engineer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 19 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You mentioned earlier that you work as a software engineer. What specific areas or technologies are you interested in within software engineering? Are you looking to learn more about AI and machine learning in the context of your job, or do you have other interests in mind?\n",
      "\n",
      "Also, I don't want to keep repeating what you've already said, so I'll try to dive a bit deeper into our conversation. What motivated you to pursue a career in software engineering?\n",
      "(Buffer size: 18 messages)\n",
      "\n",
      "--- Turn 4 ---\n",
      "User: What do I do for work?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 21 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Déjà vu! We've been here before, Alice. You mentioned earlier that you work as a software engineer. If you'd like to explore more about your job or discuss any related topics, I'm here to help.\n",
      "\n",
      "However, if you're genuinely curious about what a software engineer does, I can try to provide some general information. Software engineers design, develop, and test software programs for various industries and applications. They work on writing code, debugging, and collaborating with teams to deliver high-quality software solutions.\n",
      "\n",
      "But since we've already had this conversation before, I'm curious - is there something specific you'd like to talk about related to your job or work in general?\n",
      "(Buffer size: 20 messages)\n",
      "\n",
      "--- Turn 5 ---\n",
      "User: I'm interested in AI and machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 23 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: We've been here before, Alice! You mentioned earlier that you're interested in AI and machine learning. That's a fascinating area of study!\n",
      "\n",
      "Since we've had this conversation before, I want to make sure I understand what draws you to AI and machine learning. Are you looking to apply these concepts to your current work as a software engineer or explore them in a different context?\n",
      "\n",
      "Also, are there any specific aspects of AI and machine learning that interest you the most? For example, natural language processing, computer vision, or perhaps reinforcement learning?\n",
      "(Buffer size: 22 messages)\n",
      "\n",
      "--- Turn 6 ---\n",
      "User: What are my interests?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 25 messages in buffer\n",
      "INFO:__main__:Trimmed buffer to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You mentioned earlier that you're interested in AI and machine learning. That's a great starting point!\n",
      "\n",
      "Let me try to recap our conversation so far: You work as a software engineer and have expressed interest in AI and machine learning.\n",
      "\n",
      "Is there anything else you'd like to add or discuss about your interests? Or would you like to explore topics related to AI and machine learning further?\n",
      "\n",
      "(By the way, I'll keep in mind that we've had this conversation before, but feel free to correct me if my memory is a bit fuzzy!)\n",
      "(Buffer size: 24 messages)\n",
      "\n",
      "--- Turn 7 ---\n",
      "User: do you remember my name?\n",
      "Agent: You mentioned earlier that your name is Alice. I'm glad I could recall it correctly! Would you like to continue our conversation with a fresh start or revisit something we discussed earlier, Alice?\n",
      "(Buffer size: 26 messages)\n"
     ]
    }
   ],
   "source": [
    "session = \"user-123\"\n",
    "    \n",
    "conversations = [\n",
    "    \"Hi, my name is Alice\",\n",
    "    \"What's my name?\",\n",
    "    \"I work as a software engineer\",\n",
    "    \"What do I do for work?\",\n",
    "    \"I'm interested in AI and machine learning\",\n",
    "    \"What are my interests?\",\n",
    "    \"do you remember my name?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONVERSATION BUFFER DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, msg in enumerate(conversations, 1):\n",
    "    print(f\"\\n--- Turn {i} ---\")\n",
    "    print(f\"User: {msg}\")\n",
    "    \n",
    "    result = chat_with_memory(msg, session_id=session, buffer_size=10)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(f\"Agent: {result['response']}\")\n",
    "        print(f\"(Buffer size: {result['total_messages']} messages)\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bb4e036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'response': \"You mentioned earlier that your name is Alice. I'm glad I could recall it correctly! Would you like to continue our conversation with a fresh start or revisit something we discussed earlier, Alice?\",\n",
       " 'total_messages': 26}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7974a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3e65f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70c024ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = SystemMessage(\n",
    "    content=\"You are a helpful assistant with memory of the conversation. Reference previous messages when relevant.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb2477a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_trimming(state: ConversationState) -> dict:\n",
    "    \"\"\"Chat node with automatic message trimming\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get messages\n",
    "        messages = list(state[\"messages\"])\n",
    "        \n",
    "        logger.info(f\"Processing with {len(messages)} messages\")\n",
    "        \n",
    "        # Add system message if not present\n",
    "        if not messages or not isinstance(messages[0], SystemMessage):\n",
    "            messages = [SYSTEM_MESSAGE] + messages\n",
    "        \n",
    "        # Use trim_messages to intelligently manage history\n",
    "        # This keeps system message, trims middle, keeps recent messages\n",
    "        trimmed_messages = trim_messages(\n",
    "            messages,\n",
    "            max_tokens=1000,  # Keep last ~1000 tokens\n",
    "            strategy=\"last\",  # Keep most recent messages\n",
    "            token_counter=llm,  # Use LLM's token counter\n",
    "            include_system=True,  # Always keep system message\n",
    "            allow_partial=False,  # Don't split messages\n",
    "            start_on=\"human\"  # Start on human message for context\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Trimmed to {len(trimmed_messages)} messages\")\n",
    "        \n",
    "        # Invoke LLM with trimmed messages\n",
    "        response = llm.invoke(trimmed_messages)\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [response]\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat error: {e}\")\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {str(e)}\")]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68cb9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74183ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x74f42f0af2d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(ConversationState)\n",
    "workflow.add_node(\"chat\", chat_with_trimming)\n",
    "workflow.add_edge(START, \"chat\")\n",
    "workflow.add_edge(\"chat\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbe74f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"conversation_memory_new.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "conversation_agent = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62bd44d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(conversation_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f77cb444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_proper_memory(\n",
    "    message: str,\n",
    "    session_id: str = \"default\"\n",
    ") -> dict:\n",
    "    \"\"\"Chat with proper memory management using trim_messages\"\"\"\n",
    "    \n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": f\"conv-{session_id}\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=message)]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = conversation_agent.invoke(initial_state, config=config)\n",
    "        \n",
    "        final_message = result[\"messages\"][-1]\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": final_message.content,\n",
    "            \"total_messages\": len(result[\"messages\"])\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat failed: {e}\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "757c4314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 1 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROPER CONVERSATION BUFFER WITH trim_messages\n",
      "============================================================\n",
      "\n",
      "--- Turn 1 ---\n",
      "User: Hi, my name is Alice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Trimmed to 2 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing with 3 messages\n",
      "INFO:__main__:Trimmed to 4 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing with 5 messages\n",
      "INFO:__main__:Trimmed to 6 messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Hello Alice! It's nice to meet you. This is our first conversation, so I don't have any prior knowledge about you. How can I assist you today?\n",
      "(Total messages in state: 2)\n",
      "\n",
      "--- Turn 2 ---\n",
      "User: What's my name?\n",
      "Agent: You mentioned your name earlier as \"Alice\". Is there something specific related to your name that brings you here today?\n",
      "(Total messages in state: 4)\n",
      "\n",
      "--- Turn 3 ---\n",
      "User: I work as a software engineer at Google\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing with 7 messages\n",
      "INFO:__main__:Trimmed to 8 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You're working as a software engineer at Google, which is one of the largest and most influential tech companies in the world. That's impressive! What aspects of software engineering do you focus on, or are there any specific projects or areas that interest you?\n",
      "(Total messages in state: 6)\n",
      "\n",
      "--- Turn 4 ---\n",
      "User: What do I do for work?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 9 messages\n",
      "INFO:__main__:Trimmed to 10 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You mentioned earlier that you're a software engineer at Google. To recap, you're a professional who designs, develops, and tests software programs, often working on complex technical problems to create innovative solutions for companies like Google. Does that sound about right?\n",
      "(Total messages in state: 8)\n",
      "\n",
      "--- Turn 5 ---\n",
      "User: I'm interested in AI and machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 11 messages\n",
      "INFO:__main__:Trimmed to 12 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: That's a fascinating area of focus! As a software engineer at Google, you likely have opportunities to work with various AI and ML technologies, such as natural language processing, computer vision, or predictive analytics.\n",
      "\n",
      "Are you looking to dive deeper into any specific aspect of AI/ML, like building conversational interfaces, developing recommender systems, or exploring the latest advancements in deep learning?\n",
      "(Total messages in state: 10)\n",
      "\n",
      "--- Turn 6 ---\n",
      "User: What are my interests?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 13 messages\n",
      "INFO:__main__:Trimmed to 14 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You mentioned earlier that you're interested in Artificial Intelligence (AI) and Machine Learning (ML). That's a broad and exciting field with many potential applications. You also work as a software engineer at Google, which suggests that you have a strong technical background.\n",
      "\n",
      "To recap, your interests include:\n",
      "\n",
      "1. AI\n",
      "2. ML\n",
      "\n",
      "Is there anything specific within these areas that captures your attention?\n",
      "(Total messages in state: 12)\n",
      "\n",
      "--- Turn 7 ---\n",
      "User: I live in San Francisco\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing with 15 messages\n",
      "INFO:__main__:Trimmed to 16 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Processing with 17 messages\n",
      "INFO:__main__:Trimmed to 18 messages\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You mentioned earlier that you work as a software engineer at Google, and now I see that you also live in San Francisco! That's great - living in the heart of Silicon Valley can be very convenient for tech professionals like yourself.\n",
      "\n",
      "San Francisco is known for its vibrant tech community, with many startups and established companies like Google calling it home. Does your location play a role in your work or interests at all?\n",
      "(Total messages in state: 14)\n",
      "\n",
      "--- Turn 8 ---\n",
      "User: Where do I live?\n",
      "Agent: You mentioned earlier that you live in San Francisco!\n",
      "(Total messages in state: 16)\n",
      "\n",
      "--- Turn 9 ---\n",
      "User: Do you remember my name\n",
      "Agent: Your name is Alice, and we also discussed that you're a software engineer working at Google, with an interest in Artificial Intelligence (AI) and Machine Learning (ML). And, of course, you live in San Francisco. Is there anything else I should remember?\n",
      "(Total messages in state: 18)\n"
     ]
    }
   ],
   "source": [
    "session = \"user-1234\"\n",
    "    \n",
    "conversations = [\n",
    "    \"Hi, my name is Alice\",\n",
    "    \"What's my name?\",\n",
    "    \"I work as a software engineer at Google\",\n",
    "    \"What do I do for work?\",\n",
    "    \"I'm interested in AI and machine learning\",\n",
    "    \"What are my interests?\",\n",
    "    \"I live in San Francisco\",\n",
    "    \"Where do I live?\",\n",
    "    \"Do you remember my name\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROPER CONVERSATION BUFFER WITH trim_messages\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, msg in enumerate(conversations, 1):\n",
    "    print(f\"\\n--- Turn {i} ---\")\n",
    "    print(f\"User: {msg}\")\n",
    "    \n",
    "    result = chat_with_proper_memory(msg, session_id=session)\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        print(f\"Agent: {result['response']}\")\n",
    "        print(f\"(Total messages in state: {result['total_messages']})\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca621e",
   "metadata": {},
   "source": [
    "### Summarization pattern for short term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "996cd25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "691f427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b663dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"\"\"Summarize this conversation concisely, preserving key information:\n",
    "     {conversation}\n",
    "     provide a 2-3 sentence summary: \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "114949f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_chain = summarize_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a0c1567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_summary(state: SummarizationState) -> dict:\n",
    "    \"\"\"Chat using summary + recent messages\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        m for m in state[\"messages\"]\n",
    "        if isinstance(m, (HumanMessage, AIMessage, SystemMessage))\n",
    "    ]\n",
    "    \n",
    "    # Build context with summary\n",
    "    context_messages = []\n",
    "    \n",
    "    if state.get(\"summary\"):\n",
    "        context_messages.append(\n",
    "            SystemMessage(content=f\"Conversation summary: {state['summary']}\")\n",
    "        )\n",
    "    \n",
    "    context_messages.extend(messages)\n",
    "    \n",
    "    # Invoke LLM\n",
    "    response = llm.invoke(context_messages)\n",
    "    \n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9912ca90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_and_trim(state: SummarizationState) -> dict:\n",
    "    \"\"\"Summarize old messages and trim\"\"\"\n",
    "\n",
    "    messages = list(state[\"messages\"])\n",
    "\n",
    "    if len(messages) < 10:\n",
    "        return {}\n",
    "    \n",
    "    logger.info(\"creating summary and trimming messages...\")\n",
    "\n",
    "    # summarize last 4 messages and remove those later\n",
    "    old_messages = messages[:-4]\n",
    "\n",
    "    conversation_text = \"\\n\".join([\n",
    "        f\"{'User' if isinstance(m, HumanMessage) else 'Assistant'}: {m.content}\"\n",
    "        for m in old_messages\n",
    "    ])\n",
    "\n",
    "    summary_response = summarize_chain.invoke({\"conversation\": conversation_text})\n",
    "    new_summary = summary_response.content\n",
    "\n",
    "    if state.get(\"summary\"):\n",
    "        combined_summary = f\"{state['summary']}\\n\\nRecent: {new_summary}\"\n",
    "    else:\n",
    "        combined_summary = new_summary\n",
    "\n",
    "    \n",
    "    logger.info(f\"Created summary: {new_summary[:100]}...\")\n",
    "    \n",
    "    # Remove old messages\n",
    "    messages_to_remove = [RemoveMessage(id=m.id) for m in old_messages]\n",
    "    \n",
    "    return {\n",
    "        \"summary\": combined_summary,\n",
    "        \"messages\": messages_to_remove\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cbdfddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(SummarizationState)\n",
    "workflow.add_node(\"chat\", chat_with_summary)\n",
    "workflow.add_node(\"summarize\", summarize_and_trim)\n",
    "\n",
    "workflow.set_entry_point(\"chat\")\n",
    "workflow.add_edge(\"chat\", \"summarize\")\n",
    "workflow.add_edge(\"summarize\", END)\n",
    "\n",
    "conn = sqlite3.connect(\"summarization_memory.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "summarization_agent = workflow.compile(\n",
    "    checkpointer=checkpointer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cdcb06b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(summarization_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a83ba01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_summarization(message: str, session_id: str = \"default\") -> dict:\n",
    "    \"\"\"Chat with automatic summarization\"\"\"\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": f\"sum-{session_id}\"}}\n",
    "    \n",
    "    result = summarization_agent.invoke(\n",
    "        {\"messages\": [HumanMessage(content=message)], \"summary\": \"\"},\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"response\": result[\"messages\"][-1].content,\n",
    "        \"summary\": result.get(\"summary\", \"\"),\n",
    "        \"message_count\": len(result[\"messages\"])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b044da80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARIZATION PATTERN\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: Here is a concise summary of our conversation:\n",
      "\n",
      "The conversation started with the user providing som...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After message 1:\n",
      "  Messages in buffer: 34\n",
      "  Has summary: True\n",
      "  Summary: Here is a concise summary of our conversation:\n",
      "\n",
      "The conversation started with the user providing som...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: Here is a concise summary of the conversation:\n",
      "\n",
      "The user sent a series of messages to keep track of ...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: Here is a concise summary of the conversation:\n",
      "\n",
      "The user sent a series of messages with content to r...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: This conversation started with the user providing content to remember in each message, and the assis...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: The conversation started with a series of messages where the assistant kept track of \"content to rem...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: The conversation involved a repeated process of sending and receiving messages with \"content to reme...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After message 6:\n",
      "  Messages in buffer: 1088\n",
      "  Has summary: True\n",
      "  Summary: The conversation involved a repeated process of sending and receiving messages with \"content to reme...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: Here is a 2-3 sentence summary:\n",
      "\n",
      "This conversation started with the user providing a series of messa...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: It looks like there was an error in the input. The text seems to be incomplete and doesn't contain a...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: I can't fulfill this request....\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: I'm happy to help, but it seems like there was an error in your request. You provided a prompt with ...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: I'm happy to help, but it seems there was an error in your request. You didn't provide any text for ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After message 11:\n",
      "  Messages in buffer: 34816\n",
      "  Has summary: True\n",
      "  Summary: I'm happy to help, but it seems there was an error in your request. You didn't provide any text for ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: I can help you with that. However, I don't see any text to summarize. Could you please provide the t...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: There is no text to summarize. Please provide the text you would like me to summarize, and I will be...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: I don't see any text to summarize. Please provide the text you would like me to summarize, and I wil...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:creating summary and trimming messages...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created summary: I'm happy to help, but it seems there was an error in your request. You asked me to provide a 2-3 se...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARIZATION PATTERN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "session = \"sum-test\"\n",
    "\n",
    "# Long conversation\n",
    "for i in range(15):\n",
    "    msg = f\"This is message {i+1} with some content to remember\"\n",
    "    result = chat_with_summarization(msg, session_id=session)\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        print(f\"\\nAfter message {i+1}:\")\n",
    "        print(f\"  Messages in buffer: {result['message_count']}\")\n",
    "        print(f\"  Has summary: {bool(result['summary'])}\")\n",
    "        if result['summary']:\n",
    "            print(f\"  Summary: {result['summary'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d492aa",
   "metadata": {},
   "source": [
    "### Long term memory with vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7911c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from typing import TypedDict, Annotated, Sequence, List\n",
    "from langchain_core.documents import Document\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d1642ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorMemoryState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add]\n",
    "    retrieved_memories: List[str]\n",
    "    user_id: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "041b3207",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.7)\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3372eca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX512 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX512 support.\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Created new vector store\n"
     ]
    }
   ],
   "source": [
    "# Create vector store (initialize once)\n",
    "try:\n",
    "    vector_store = FAISS.load_local(\n",
    "        \"./faiss_memory\",\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "    logger.info(\"Loaded existing vector store\")\n",
    "except:\n",
    "    # Create new vector store\n",
    "    initial_docs = [\n",
    "        Document(\n",
    "            page_content=\"System initialized\",\n",
    "            metadata={\"timestamp\": datetime.now().isoformat(), \"type\": \"system\"}\n",
    "        )\n",
    "    ]\n",
    "    vector_store = FAISS.from_documents(initial_docs, embeddings)\n",
    "    vector_store.save_local(\"./faiss_memory\")\n",
    "    logger.info(\"Created new vector store\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7455256",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SYSTEM_PROMPT = \"\"\"You are a helpful assistant with long-term memory.\n",
    "\n",
    "Retrieved relevant memories:\n",
    "{memories}\n",
    "\n",
    "Use these memories to provide context-aware responses. Reference past conversations naturally.\"\"\"\n",
    "\n",
    "memory_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", MEMORY_SYSTEM_PROMPT),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "memory_chain = memory_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78debb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_memories(state: VectorMemoryState) -> dict:\n",
    "    \"\"\"Retrieve relevant memories from vector store\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get current message\n",
    "        current_message = state[\"messages\"][-1].content\n",
    "        \n",
    "        logger.info(f\"Retrieving memories for: {current_message[:50]}...\")\n",
    "        \n",
    "        # Search vector store\n",
    "        docs = vector_store.similarity_search(\n",
    "            current_message,\n",
    "            k=3,  # Top 3 most relevant memories\n",
    "            filter={\"user_id\": state[\"user_id\"]} if state.get(\"user_id\") else None\n",
    "        )\n",
    "        \n",
    "        # Format memories\n",
    "        memories = []\n",
    "        for doc in docs:\n",
    "            timestamp = doc.metadata.get(\"timestamp\", \"unknown\")\n",
    "            memory_type = doc.metadata.get(\"type\", \"conversation\")\n",
    "            memories.append(f\"[{timestamp}] ({memory_type}): {doc.page_content}\")\n",
    "        \n",
    "        logger.info(f\"Retrieved {len(memories)} memories\")\n",
    "        \n",
    "        return {\n",
    "            \"retrieved_memories\": memories\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Memory retrieval error: {e}\")\n",
    "        return {\n",
    "            \"retrieved_memories\": []\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c96642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def respond_with_memory(state: VectorMemoryState) -> dict:\n",
    "    \"\"\"Generate response using retrieved memories\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Format memories\n",
    "        memories_text = \"\\n\".join(state[\"retrieved_memories\"]) if state[\"retrieved_memories\"] else \"No relevant memories found.\"\n",
    "        \n",
    "        # Invoke LLM\n",
    "        response = memory_chain.invoke({\n",
    "            \"memories\": memories_text,\n",
    "            \"messages\": state[\"messages\"]\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [response]\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Response generation error: {e}\")\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {str(e)}\")]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6bf54b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_memory(state: VectorMemoryState) -> dict:\n",
    "    \"\"\"Store important information in long-term memory\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get last few messages\n",
    "        recent_messages = state[\"messages\"][-2:]  # Last user message and AI response\n",
    "        \n",
    "        # Extract user message and AI response\n",
    "        user_msg = None\n",
    "        ai_msg = None\n",
    "        \n",
    "        for msg in recent_messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                user_msg = msg.content\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                ai_msg = msg.content\n",
    "        \n",
    "        if user_msg and ai_msg:\n",
    "            # Check if this is important to remember\n",
    "            # Simple heuristic: if user shares personal info\n",
    "            important_keywords = [\"my name is\", \"i am\", \"i work\", \"i like\", \"i live\", \"i have\"]\n",
    "            \n",
    "            should_store = any(keyword in user_msg.lower() for keyword in important_keywords)\n",
    "            \n",
    "            if should_store:\n",
    "                # Create memory document\n",
    "                memory_content = f\"User: {user_msg}\\nAssistant: {ai_msg}\"\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=memory_content,\n",
    "                    metadata={\n",
    "                        \"timestamp\": datetime.now().isoformat(),\n",
    "                        \"user_id\": state.get(\"user_id\", \"unknown\"),\n",
    "                        \"type\": \"conversation\"\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Add to vector store\n",
    "                vector_store.add_documents([doc])\n",
    "                vector_store.save_local(\"./faiss_memory\")\n",
    "                \n",
    "                logger.info(\"Stored new memory\")\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Memory storage error: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "828616ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(VectorMemoryState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve_memories)\n",
    "workflow.add_node(\"respond\", respond_with_memory)\n",
    "workflow.add_node(\"store\", store_memory)\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"respond\")\n",
    "workflow.add_edge(\"respond\", \"store\")\n",
    "workflow.add_edge(\"store\", END)\n",
    "\n",
    "# Compile\n",
    "conn = sqlite3.connect(\"vector_memory_agent.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "vector_memory_agent = workflow.compile(checkpointer=checkpointer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "caaadaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_vector_memory(\n",
    "    message: str,\n",
    "    user_id: str = \"default\"\n",
    ") -> dict:\n",
    "    \"\"\"Chat with long-term vector memory\"\"\"\n",
    "    \n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": f\"vector-mem-{user_id}\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "        \"retrieved_memories\": [],\n",
    "        \"user_id\": user_id\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = vector_memory_agent.invoke(initial_state, config=config)\n",
    "        \n",
    "        final_message = result[\"messages\"][-1]\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": final_message.content,\n",
    "            \"memories_retrieved\": len(result[\"retrieved_memories\"])\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat failed: {e}\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0b63e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(vector_memory_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "24a48463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Retrieving memories for: Hi, my name is Alice and I'm a software engineer...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Retrieved 2 memories\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VECTOR MEMORY DEMO - First Conversation\n",
      "============================================================\n",
      "\n",
      "User: Hi, my name is Alice and I'm a software engineer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Stored new memory\n",
      "INFO:__main__:Retrieving memories for: I work primarily with Python and I love building A...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Retrieved 3 memories\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Nice to meet you, Alice! It's great to have you here. As we previously discussed, I recall that you're primarily working with Python and have a passion for building AI applications.\n",
      "\n",
      "I'd love to catch up on what's new in your world. Are you still exploring the latest developments in machine learning and deep learning? Have you come across any interesting projects or challenges recently?\n",
      "\n",
      "If not, we can always start from scratch and discuss the latest trends and technologies in the field. I'm here to help and provide guidance whenever you need it.\n",
      "\n",
      "What's been on your mind lately, Alice?\n",
      "\n",
      "User: I work primarily with Python and I love building AI applications\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Stored new memory\n",
      "INFO:__main__:Retrieving memories for: My favorite hobby is rock climbing...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Retrieved 3 memories\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: A software engineer with a passion for AI! That's fascinating, Alice. As we previously discussed, I recall our conversation when you mentioned your interest in building AI applications.\n",
      "\n",
      "Python is an excellent language to work with, especially for machine learning and deep learning tasks. Have you had any experiences with popular Python libraries like TensorFlow, Keras, or scikit-learn? Or perhaps you're interested in exploring newer frameworks like PyTorch or Hugging Face?\n",
      "\n",
      "What specific area of AI are you most interested in? Natural Language Processing (NLP), Computer Vision, Reinforcement Learning, or maybe something else? I'm here to help and would love to discuss your projects or provide guidance on any challenges you're facing.\n",
      "\n",
      "By the way, what kind of AI applications have you been working on recently? Are there any specific use cases or industries that interest you?\n",
      "\n",
      "User: My favorite hobby is rock climbing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Retrieving memories for: What's my name?...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Retrieved 3 memories\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Rock climbing! That's a fantastic hobby, Alice. It requires physical strength, mental focus, and problem-solving skills - all qualities that are also essential for building AI applications.\n",
      "\n",
      "I'm curious, how do you think the skills you've developed through rock climbing might translate to your work in AI? For example, the ability to approach complex problems from different angles or the need to stay calm under pressure could be valuable assets in machine learning and deep learning.\n",
      "\n",
      "Do you find that your experience with rock climbing has influenced any of your projects or approaches to AI applications?\n",
      "\n",
      "============================================================\n",
      "VECTOR MEMORY DEMO - Later Conversation\n",
      "============================================================\n",
      "\n",
      "User: What's my name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Retrieving memories for: What do I do for work?...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Retrieved 3 memories\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Your name is Alice, a software engineer with a passion for building AI applications. I've been chatting with you about various topics, including your love for Python and machine learning, as well as your hobby of rock climbing. Is there anything else you'd like to talk about or ask about?\n",
      "(Retrieved 3 relevant memories)\n",
      "\n",
      "User: What do I do for work?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Retrieving memories for: What are my hobbies?...\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Retrieved 3 memories\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You're a software engineer, Alice! You primarily work with Python and have a passion for building AI applications. You've been exploring the latest developments in machine learning and deep learning, and we chatted about some of the projects you've been working on and your interests in different areas of AI. How can I assist you further today?\n",
      "(Retrieved 3 relevant memories)\n",
      "\n",
      "User: What are my hobbies?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You enjoy rock climbing as a hobby, Alice! It's great to have something outside of work that brings you joy and challenges you in new ways. Do you have a favorite climbing spot or a particularly memorable climb that stands out to you?\n",
      "(Retrieved 3 relevant memories)\n"
     ]
    }
   ],
   "source": [
    "user = \"alice-123\"\n",
    "    \n",
    "# First conversation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VECTOR MEMORY DEMO - First Conversation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "messages_1 = [\n",
    "    \"Hi, my name is Alice and I'm a software engineer\",\n",
    "    \"I work primarily with Python and I love building AI applications\",\n",
    "    \"My favorite hobby is rock climbing\"\n",
    "]\n",
    "\n",
    "for msg in messages_1:\n",
    "    print(f\"\\nUser: {msg}\")\n",
    "    result = chat_with_vector_memory(msg, user_id=user)\n",
    "    if result[\"success\"]:\n",
    "        print(f\"Agent: {result['response']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VECTOR MEMORY DEMO - Later Conversation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Later conversation - test memory recall\n",
    "messages_2 = [\n",
    "    \"What's my name?\",\n",
    "    \"What do I do for work?\",\n",
    "    \"What are my hobbies?\"\n",
    "]\n",
    "\n",
    "for msg in messages_2:\n",
    "    print(f\"\\nUser: {msg}\")\n",
    "    result = chat_with_vector_memory(msg, user_id=user)\n",
    "    if result[\"success\"]:\n",
    "        print(f\"Agent: {result['response']}\")\n",
    "        print(f\"(Retrieved {result['memories_retrieved']} relevant memories)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16fcfff",
   "metadata": {},
   "source": [
    "### Enterprise based memory architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39aade14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alokkumar/langgraph-tutorial/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, Sequence, List, Dict, Optional, Literal\n",
    "from operator import add\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from datetime import datetime, timedelta\n",
    "from dataclasses import dataclass, asdict\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ffb9130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from IPython.display import display, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4849c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef87dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data models\n",
    "@dataclass\n",
    "class Memory:\n",
    "    \"\"\"structured memory object\"\"\"\n",
    "    content: str\n",
    "    memory_type: Literal[\"semantic\", \"episodic\", \"procedural\"]\n",
    "    importance: float # 0 to 1\n",
    "    timestamp: str\n",
    "    user_id: str\n",
    "    metadata: Dict\n",
    "\n",
    "    def to_document(self) -> Document:\n",
    "        \"\"\"convert to langchain document\"\"\"\n",
    "        return Document(\n",
    "            page_content=self.content,\n",
    "            metadata={\n",
    "                \"memory_type\": self.memory_type,\n",
    "                \"importance\": self.importance,\n",
    "                \"timestamp\": self.timestamp,\n",
    "                \"user_id\": self.user_id,\n",
    "                **self.metadata\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def from_document(cls, doc: Document) -> \"Memory\":\n",
    "        \"\"\"create from langchain document\"\"\"\n",
    "        metadata = doc.metadata.copy()\n",
    "        return cls(\n",
    "            content=doc.page_content,\n",
    "            memory_type=metadata.pop(\"memory_type\", \"semantic\"),\n",
    "            importance=metadata.pop(\"importance\", 0.5),\n",
    "            timestamp=metadata.pop(\"timestamp\", datetime.now().isoformat()),\n",
    "            user_id=metadata.pop(\"user_id\", \"unknown\"),\n",
    "            metadata=metadata\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d65fdaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state\n",
    "class ProductionMemoryState(TypedDict):\n",
    "    \"\"\"complete production memory state\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add]\n",
    "\n",
    "    # Memory retrieval\n",
    "    retrieved_memories: List[Memory]\n",
    "    memory_query: str\n",
    "\n",
    "    # User context\n",
    "    user_id: str\n",
    "    session_id: str\n",
    "    \n",
    "    # Memory management\n",
    "    buffer_size: int\n",
    "    compression_threshold: int\n",
    "    conversation_summary: str\n",
    "    \n",
    "    # Metadata\n",
    "    turn_count: Annotated[int, add]\n",
    "    total_memories_stored: Annotated[int, add]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b327d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryManager:\n",
    "    \"\"\"Centralized memory management system\"\"\"\n",
    "    def __init__(self, storage_path: str=\"production_memory\"):\n",
    "        self.storage_path = Path(storage_path)\n",
    "        self.storage_path.mkdir(exist_ok=True)\n",
    "\n",
    "        self.embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "        \n",
    "        self.stores = {}\n",
    "        for memory_type in [\"semantic\", \"episodic\", \"procedural\"]:\n",
    "            store_path = self.storage_path / memory_type\n",
    "            try:\n",
    "                self.stores[memory_type] = FAISS.load_local(\n",
    "                    str(store_path),\n",
    "                    self.embeddings,\n",
    "                    allow_dangerous_deserialization=True\n",
    "                )\n",
    "                logger.info(f\"Loaded {memory_type} memory store\")\n",
    "            except:\n",
    "                # Create new store\n",
    "                init_doc = [Document(\n",
    "                    page_content=f\"{memory_type} memory initialized\",\n",
    "                    metadata={\n",
    "                        \"timestamp\": datetime.now().isoformat(),\n",
    "                        \"memory_type\": memory_type\n",
    "                    }\n",
    "                )]\n",
    "                self.stores[memory_type] = FAISS.from_documents(init_doc, self.embeddings)\n",
    "                self.stores[memory_type].save_local(str(store_path))\n",
    "                logger.info(f\"Created {memory_type} memory store\")\n",
    "\n",
    "    def store_memory(self, memory: Memory) -> None:\n",
    "        \"\"\"Store memory in appropriate vector store\"\"\"\n",
    "        try:\n",
    "            store = self.stores[memory.memory_type]\n",
    "            doc = memory.to_document()\n",
    "            store.add_documents([doc])\n",
    "            \n",
    "            # Save to disk\n",
    "            store_path = self.storage_path / memory.memory_type\n",
    "            store.save_local(str(store_path))\n",
    "            \n",
    "            logger.info(f\"Stored {memory.memory_type} memory for user {memory.user_id}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error storing memory: {e}\")\n",
    "\n",
    "    def retrieve_memories(\n",
    "        self,\n",
    "        query: str,\n",
    "        user_id: str,\n",
    "        memory_types: List[str] = None,\n",
    "        k: int = 5,\n",
    "        importance_threshold: float = 0.3\n",
    "    ) -> List[Memory]:\n",
    "        \"\"\"Retrieve relevant memories\"\"\"\n",
    "        \n",
    "        if memory_types is None:\n",
    "            memory_types = [\"semantic\", \"episodic\", \"procedural\"]\n",
    "        \n",
    "        all_memories = []\n",
    "        \n",
    "        for memory_type in memory_types:\n",
    "            if memory_type not in self.stores:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                store = self.stores[memory_type]\n",
    "                \n",
    "                # Search with filters\n",
    "                docs = store.similarity_search(\n",
    "                    query,\n",
    "                    k=k,\n",
    "                    filter={\"user_id\": user_id}\n",
    "                )\n",
    "                \n",
    "                # Convert to Memory objects\n",
    "                memories = [Memory.from_document(doc) for doc in docs]\n",
    "                \n",
    "                # Filter by importance\n",
    "                memories = [m for m in memories if m.importance >= importance_threshold]\n",
    "                \n",
    "                all_memories.extend(memories)\n",
    "            \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error retrieving {memory_type} memories: {e}\")\n",
    "        \n",
    "        # Sort by importance and recency\n",
    "        now = datetime.now()\n",
    "        \n",
    "        def score_memory(mem: Memory) -> float:\n",
    "            \"\"\"Calculate memory relevance score\"\"\"\n",
    "            try:\n",
    "                timestamp = datetime.fromisoformat(mem.timestamp)\n",
    "                days_old = (now - timestamp).days\n",
    "                recency_factor = 1.0 / (1.0 + days_old * 0.1)\n",
    "                \n",
    "                return mem.importance * 0.6 + recency_factor * 0.4\n",
    "            except:\n",
    "                return mem.importance\n",
    "        \n",
    "        all_memories.sort(key=score_memory, reverse=True)\n",
    "        \n",
    "        return all_memories[:k]\n",
    "    \n",
    "    def cleanup_old_memories(\n",
    "        self,\n",
    "        user_id: str,\n",
    "        days_old: int = 90,\n",
    "        importance_threshold: float = 0.5\n",
    "    ):\n",
    "        \"\"\"Remove old, unimportant memories\"\"\"\n",
    "        # This is a simplified version\n",
    "        # In production, you'd implement proper cleanup logic\n",
    "        logger.info(f\"Cleanup old memories for user {user_id}\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7edfc46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MemoryAnalyzer:\n",
    "    \"\"\"Analyze conversations and extract memories\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOllama(model=\"llama3.2\", temperature=0.3)\n",
    "    \n",
    "    def analyze_conversation(\n",
    "        self,\n",
    "        user_message: str,\n",
    "        ai_message: str\n",
    "    ) -> List[Memory]:\n",
    "        \"\"\"Analyze conversation and extract memories\"\"\"\n",
    "        \n",
    "        analysis_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"human\", \"\"\"Analyze this conversation and extract important information to remember.\n",
    "\n",
    "User: {user_message}\n",
    "Assistant: {ai_message}\n",
    "\n",
    "Extract:\n",
    "1. Facts (semantic memory) - general knowledge, definitions\n",
    "2. Personal info (episodic memory) - user's experiences, preferences\n",
    "3. Procedures (procedural memory) - how to do things, workflows\n",
    "\n",
    "For each item, provide:\n",
    "- content: what to remember\n",
    "- type: semantic, episodic, or procedural\n",
    "- importance: 0.0 to 1.0\n",
    "\n",
    "Respond in JSON format:\n",
    "{{\"memories\": [{{\"content\": \"...\", \"type\": \"...\", \"importance\": 0.8}}]}}\n",
    "\n",
    "Analysis:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            chain = analysis_prompt | self.llm\n",
    "            response = chain.invoke({\n",
    "                \"user_message\": user_message,\n",
    "                \"ai_message\": ai_message\n",
    "            })\n",
    "            \n",
    "            # Parse JSON\n",
    "            response_text = response.content.strip()\n",
    "            \n",
    "            # Remove markdown code blocks if present\n",
    "            if \"```json\" in response_text:\n",
    "                response_text = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "            elif \"```\" in response_text:\n",
    "                response_text = response_text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "            \n",
    "            data = json.loads(response_text)\n",
    "            \n",
    "            memories = []\n",
    "            for item in data.get(\"memories\", []):\n",
    "                memory = Memory(\n",
    "                    content=item[\"content\"],\n",
    "                    memory_type=item[\"type\"],\n",
    "                    importance=item[\"importance\"],\n",
    "                    timestamp=datetime.now().isoformat(),\n",
    "                    user_id=\"\",  # Will be set by caller\n",
    "                    metadata={}\n",
    "                )\n",
    "                memories.append(memory)\n",
    "            \n",
    "            logger.info(f\"Extracted {len(memories)} memories\")\n",
    "            return memories\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Memory analysis error: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bde6f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummarizer:\n",
    "    \"\"\"Summarize long conversations\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = ChatOllama(model=\"llama3.2\", temperature=0.3)\n",
    "    \n",
    "    def summarize(self, messages: Sequence[BaseMessage]) -> str:\n",
    "        \"\"\"Create conversation summary\"\"\"\n",
    "        \n",
    "        if len(messages) < 5:\n",
    "            return \"\"\n",
    "        \n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"human\", \"\"\"Summarize this conversation, preserving key facts and context:\n",
    "\n",
    "{conversation}\n",
    "\n",
    "Provide a concise 2-3 sentence summary.\n",
    "\n",
    "Summary:\"\"\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            # Format conversation\n",
    "            conv_text = \"\\n\".join([\n",
    "                f\"{'User' if isinstance(m, HumanMessage) else 'Assistant'}: {m.content}\"\n",
    "                for m in messages\n",
    "            ])\n",
    "            \n",
    "            chain = summary_prompt | self.llm\n",
    "            response = chain.invoke({\"conversation\": conv_text})\n",
    "            \n",
    "            return response.content.strip()\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Summarization error: {e}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e5831f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 14:53:07,013 - faiss.loader - INFO - Loading faiss.\n",
      "2026-01-04 14:53:07,349 - faiss.loader - INFO - Successfully loaded faiss.\n",
      "2026-01-04 14:53:07,361 - __main__ - INFO - Loaded semantic memory store\n",
      "2026-01-04 14:53:07,362 - __main__ - INFO - Loaded episodic memory store\n",
      "2026-01-04 14:53:07,362 - __main__ - INFO - Loaded procedural memory store\n"
     ]
    }
   ],
   "source": [
    "memory_manager = MemoryManager()\n",
    "memory_analyzer = MemoryAnalyzer()\n",
    "conversation_summarizer = ConversationSummarizer()\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beb4890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_memories(state: ProductionMemoryState) -> dict:\n",
    "    \"\"\"Retrieve relevant memories for current query\"\"\"\n",
    "    \n",
    "    try:\n",
    "        current_message = state[\"messages\"][-1].content\n",
    "        user_id = state[\"user_id\"]\n",
    "        \n",
    "        logger.info(f\"Retrieving memories for user {user_id}\")\n",
    "        \n",
    "        # Retrieve memories\n",
    "        memories = memory_manager.retrieve_memories(\n",
    "            query=current_message,\n",
    "            user_id=user_id,\n",
    "            k=5\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"retrieved_memories\": memories,\n",
    "            \"memory_query\": current_message\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Memory retrieval error: {e}\")\n",
    "        return {\n",
    "            \"retrieved_memories\": [],\n",
    "            \"memory_query\": \"\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ba82cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_conversation_buffer(state: ProductionMemoryState) -> dict:\n",
    "    \"\"\"Manage conversation buffer with summarization\"\"\"\n",
    "    \n",
    "    try:\n",
    "        messages = state[\"messages\"]\n",
    "        buffer_size = state.get(\"buffer_size\", 10)\n",
    "        threshold = state.get(\"compression_threshold\", 20)\n",
    "        \n",
    "        # Check if summarization needed\n",
    "        if len(messages) >= threshold:\n",
    "            logger.info(\"Conversation threshold reached - creating summary\")\n",
    "            \n",
    "            # Summarize old messages\n",
    "            old_messages = messages[:-buffer_size]\n",
    "            summary = conversation_summarizer.summarize(old_messages)\n",
    "            \n",
    "            # Keep only recent messages\n",
    "            recent_messages = messages[-buffer_size:]\n",
    "            \n",
    "            return {\n",
    "                \"conversation_summary\": summary,\n",
    "                \"messages\": recent_messages\n",
    "            }\n",
    "        \n",
    "        return {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Buffer management error: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d62bee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(state: ProductionMemoryState) -> dict:\n",
    "    \"\"\"Generate response using memories\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Format memories\n",
    "        memories_text = []\n",
    "        \n",
    "        if state.get(\"conversation_summary\"):\n",
    "            memories_text.append(f\"Previous conversation: {state['conversation_summary']}\")\n",
    "        \n",
    "        for memory in state.get(\"retrieved_memories\", []):\n",
    "            mem_type = memory.memory_type\n",
    "            importance = memory.importance\n",
    "            content = memory.content\n",
    "            memories_text.append(f\"[{mem_type}, importance: {importance:.1f}] {content}\")\n",
    "        \n",
    "        memories_str = \"\\n\".join(memories_text) if memories_text else \"No prior context.\"\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a helpful assistant with memory of past conversations.\n",
    "\n",
    "Relevant memories and context:\n",
    "{memories}\n",
    "\n",
    "Use this context naturally to provide informed, personalized responses.\"\"\"),\n",
    "            (\"placeholder\", \"{messages}\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | llm\n",
    "        response = chain.invoke({\n",
    "            \"memories\": memories_str,\n",
    "            \"messages\": state[\"messages\"]\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"messages\": [response],\n",
    "            \"turn_count\": 1\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Response generation error: {e}\")\n",
    "        return {\n",
    "            \"messages\": [AIMessage(content=f\"Error: {str(e)}\")]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39f06255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_store_memories(state: ProductionMemoryState) -> dict:\n",
    "    \"\"\"Extract important information and store as memories\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get last exchange\n",
    "        recent_messages = state[\"messages\"][-2:]\n",
    "        \n",
    "        user_msg = None\n",
    "        ai_msg = None\n",
    "        \n",
    "        for msg in recent_messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                user_msg = msg.content\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                ai_msg = msg.content\n",
    "        \n",
    "        if not (user_msg and ai_msg):\n",
    "            return {}\n",
    "        \n",
    "        # Analyze and extract memories\n",
    "        memories = memory_analyzer.analyze_conversation(user_msg, ai_msg)\n",
    "        \n",
    "        # Set user_id and store\n",
    "        user_id = state[\"user_id\"]\n",
    "        stored_count = 0\n",
    "        \n",
    "        for memory in memories:\n",
    "            memory.user_id = user_id\n",
    "            memory_manager.store_memory(memory)\n",
    "            stored_count += 1\n",
    "        \n",
    "        logger.info(f\"Stored {stored_count} new memories\")\n",
    "        \n",
    "        return {\n",
    "            \"total_memories_stored\": stored_count\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Memory storage error: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4a211a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(ProductionMemoryState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"retrieve_memories\", retrieve_relevant_memories)\n",
    "workflow.add_node(\"manage_buffer\", manage_conversation_buffer)\n",
    "workflow.add_node(\"generate\", generate_response)\n",
    "workflow.add_node(\"store_memories\", extract_and_store_memories)\n",
    "\n",
    "# Define flow\n",
    "workflow.set_entry_point(\"retrieve_memories\")\n",
    "workflow.add_edge(\"retrieve_memories\", \"manage_buffer\")\n",
    "workflow.add_edge(\"manage_buffer\", \"generate\")\n",
    "workflow.add_edge(\"generate\", \"store_memories\")\n",
    "workflow.add_edge(\"store_memories\", END)\n",
    "\n",
    "# Compile with checkpointing\n",
    "conn = sqlite3.connect(\"production_memory_agent.db\", check_same_thread=False)\n",
    "checkpointer = SqliteSaver(conn)\n",
    "production_memory_agent = workflow.compile(checkpointer=checkpointer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f82a412f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(production_memory_agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0306cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_production_memory(\n",
    "    message: str,\n",
    "    user_id: str,\n",
    "    session_id: str = None,\n",
    "    buffer_size: int = 10,\n",
    "    compression_threshold: int = 20\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Production-ready chat with comprehensive memory.\n",
    "    \n",
    "    Args:\n",
    "        message: User message\n",
    "        user_id: User identifier\n",
    "        session_id: Session identifier (optional)\n",
    "        buffer_size: Max messages to keep in buffer\n",
    "        compression_threshold: When to compress conversation\n",
    "    \n",
    "    Returns:\n",
    "        dict with response and metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    if session_id is None:\n",
    "        session_id = f\"{user_id}-{datetime.now().strftime('%Y%m%d')}\"\n",
    "    \n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": f\"prod-mem-{session_id}\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "        \"retrieved_memories\": [],\n",
    "        \"memory_query\": \"\",\n",
    "        \"user_id\": user_id,\n",
    "        \"session_id\": session_id,\n",
    "        \"buffer_size\": buffer_size,\n",
    "        \"compression_threshold\": compression_threshold,\n",
    "        \"conversation_summary\": \"\",\n",
    "        \"turn_count\": 0,\n",
    "        \"total_memories_stored\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        result = production_memory_agent.invoke(initial_state, config=config)\n",
    "        \n",
    "        final_message = result[\"messages\"][-1]\n",
    "        \n",
    "        return {\n",
    "            \"success\": True,\n",
    "            \"response\": final_message.content,\n",
    "            \"metadata\": {\n",
    "                \"user_id\": user_id,\n",
    "                \"session_id\": session_id,\n",
    "                \"turn_count\": result[\"turn_count\"],\n",
    "                \"memories_retrieved\": len(result[\"retrieved_memories\"]),\n",
    "                \"memories_stored\": result[\"total_memories_stored\"],\n",
    "                \"has_summary\": bool(result.get(\"conversation_summary\")),\n",
    "                \"buffer_size\": len(result[\"messages\"])\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Chat failed: {e}\")\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ae20e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 15:00:31,422 - __main__ - INFO - Retrieving memories for user alice-123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PRODUCTION MEMORY SYSTEM DEMO\n",
      "============================================================\n",
      "\n",
      "--- Building Initial Memory ---\n",
      "\n",
      "User: Hi, my name is Alice and I'm a data scientist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 15:00:38,998 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:00:39,154 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:00:39,295 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:00:39,299 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:00:43,187 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:00:50,857 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:00:54,134 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:00:58,241 - __main__ - INFO - Extracted 5 memories\n",
      "2026-01-04 15:00:58,375 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:00:58,379 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:00:58,511 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:00:58,515 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:00:58,704 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:00:58,709 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:00:58,941 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:00:58,945 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:00:59,137 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:00:59,141 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:00:59,142 - __main__ - INFO - Stored 5 new memories\n",
      "2026-01-04 15:00:59,149 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:00:59,287 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: I remember! You're having trouble connecting to me earlier due to the \"[Errno 61] Connection refused\" error. But now that you've reestablished contact...\n",
      "Stats: 5 memories stored\n",
      "\n",
      "User: I work with Python and specialize in machine learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 15:00:59,421 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:00:59,553 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:00:59,556 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:01:03,716 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:13,478 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:15,438 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:20,695 - __main__ - INFO - Extracted 5 memories\n",
      "2026-01-04 15:01:20,830 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:20,834 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:01:20,971 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:20,975 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:01:21,107 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:21,111 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:01:21,244 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:21,248 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:01:21,382 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:21,387 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:01:21,387 - __main__ - INFO - Stored 5 new memories\n",
      "2026-01-04 15:01:21,395 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:01:21,533 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You're a skilled data scientist who works with Python, leveraging its strengths in machine learning to drive innovative solutions. What specific areas...\n",
      "Stats: 10 memories stored\n",
      "\n",
      "User: I'm currently learning about LangGraph and agent systems\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 15:01:21,667 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:21,801 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:21,805 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:01:26,656 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:37,417 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:41,549 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:49,418 - __main__ - INFO - Extracted 8 memories\n",
      "2026-01-04 15:01:49,553 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:49,556 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:01:49,692 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:49,696 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:01:49,829 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:49,833 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:01:49,969 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:49,973 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:01:50,105 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:50,109 - __main__ - INFO - Stored procedural memory for user alice-123\n",
      "2026-01-04 15:01:50,243 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:50,247 - __main__ - INFO - Stored procedural memory for user alice-123\n",
      "2026-01-04 15:01:50,379 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:50,384 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:01:50,517 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:50,521 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:01:50,522 - __main__ - INFO - Stored 8 new memories\n",
      "2026-01-04 15:01:50,531 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:01:50,672 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: LangGraph is a fascinating area of research that combines graph neural networks with language modeling. It has the potential to revolutionize many NLP...\n",
      "Stats: 18 memories stored\n",
      "\n",
      "User: My favorite ML framework is PyTorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 15:01:50,808 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:50,943 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:01:50,946 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:01:56,496 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:09,191 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:13,720 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:23,819 - __main__ - ERROR - Memory analysis error: Expecting property name enclosed in double quotes: line 10 column 1 (char 1023)\n",
      "2026-01-04 15:02:23,820 - __main__ - INFO - Stored 0 new memories\n",
      "2026-01-04 15:02:23,829 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:02:23,984 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: PyTorch is an excellent choice! It's a popular and powerful framework for machine learning, especially for deep learning tasks. Its dynamic computatio...\n",
      "Stats: 18 memories stored\n",
      "\n",
      "============================================================\n",
      "--- Testing Memory Recall ---\n",
      "\n",
      "User: What's my name?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 15:02:24,138 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:24,291 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:24,295 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:02:30,968 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:46,331 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:48,059 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:50,447 - __main__ - INFO - Extracted 3 memories\n",
      "2026-01-04 15:02:50,602 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:50,605 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:02:50,800 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:50,803 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:02:50,956 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:50,959 - __main__ - INFO - Stored procedural memory for user alice-123\n",
      "2026-01-04 15:02:50,960 - __main__ - INFO - Stored 3 new memories\n",
      "2026-01-04 15:02:50,968 - __main__ - INFO - Retrieving memories for user alice-123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Your name is Alice, and you're a data scientist. I've been keeping track of our conversation to make sure we pick up where we left off. How can I assist you today?\n",
      "Retrieved: 5 memories\n",
      "\n",
      "User: What do I do for work?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 15:02:51,166 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:51,361 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:51,556 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:02:51,560 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:02:59,757 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:16,173 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:18,643 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:22,263 - __main__ - INFO - Extracted 4 memories\n",
      "2026-01-04 15:03:22,399 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:22,403 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:03:22,537 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:22,541 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:03:22,693 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:22,697 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:03:22,879 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:22,883 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:03:22,884 - __main__ - INFO - Stored 4 new memories\n",
      "2026-01-04 15:03:22,893 - __main__ - INFO - Retrieving memories for user alice-123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You're a data scientist! You work with Python and specialize in machine learning, leveraging its strengths to drive innovative solutions. That's a great foundation for exploring new areas like LangGraph and agent systems. What kind of projects or applications are you interested in applying these concepts to?\n",
      "Retrieved: 5 memories\n",
      "\n",
      "User: What ML framework do I prefer?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 15:03:23,095 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:23,289 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:23,484 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:23,488 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:03:34,116 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:50,614 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:53,117 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:56,090 - __main__ - INFO - Extracted 3 memories\n",
      "2026-01-04 15:03:56,230 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:56,234 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:03:56,377 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:56,380 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:03:56,521 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:56,523 - __main__ - INFO - Stored procedural memory for user alice-123\n",
      "2026-01-04 15:03:56,524 - __main__ - INFO - Stored 3 new memories\n",
      "2026-01-04 15:03:56,537 - __main__ - INFO - Retrieving memories for user alice-123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You're a PyTorch user! You've chosen PyTorch as your preferred machine learning framework, and for good reason - it's a powerful and flexible tool that can handle a wide range of tasks, from deep learning to natural language processing. What are you most looking forward to doing with PyTorch?\n",
      "Retrieved: 5 memories\n",
      "\n",
      "User: What am I currently learning about?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 15:03:56,752 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:56,954 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:57,156 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:03:57,160 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:04:09,218 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:04:25,656 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:04:28,169 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:04:31,352 - __main__ - INFO - Extracted 3 memories\n",
      "2026-01-04 15:04:31,497 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:04:31,501 - __main__ - INFO - Stored procedural memory for user alice-123\n",
      "2026-01-04 15:04:31,641 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:04:31,646 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:04:31,852 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:04:31,856 - __main__ - INFO - Stored procedural memory for user alice-123\n",
      "2026-01-04 15:04:31,857 - __main__ - INFO - Stored 3 new memories\n",
      "2026-01-04 15:04:31,867 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:04:32,018 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: You're currently learning about LangGraph and agent systems! You've been exploring these topics and are interested in applying them to your work as a data scientist, specifically using Python and machine learning frameworks like PyTorch. Is there something specific you'd like to know or discuss about LangGraph or agent systems?\n",
      "Retrieved: 5 memories\n",
      "\n",
      "============================================================\n",
      "--- Testing Conversation Compression ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 15:04:32,160 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:04:32,300 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:04:32,304 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:04:44,935 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:00,887 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:03,889 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:10,034 - __main__ - INFO - Extracted 6 memories\n",
      "2026-01-04 15:05:10,190 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:10,195 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:05:10,352 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:10,356 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:05:10,519 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:10,524 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:05:10,691 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:10,695 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:05:10,861 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:10,863 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:05:11,030 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:11,034 - __main__ - INFO - Stored procedural memory for user alice-123\n",
      "2026-01-04 15:05:11,034 - __main__ - INFO - Stored 6 new memories\n",
      "2026-01-04 15:05:11,044 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:05:11,215 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Turn 1:\n",
      "  Buffer size: 224\n",
      "  Has summary: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 15:05:11,376 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:11,545 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:11,549 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:05:24,961 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:40,705 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:43,560 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:47,379 - __main__ - INFO - Extracted 3 memories\n",
      "2026-01-04 15:05:47,543 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:47,546 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:05:47,706 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:47,711 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:05:47,878 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:47,883 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:05:47,883 - __main__ - INFO - Stored 3 new memories\n",
      "2026-01-04 15:05:47,893 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:05:48,071 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:48,240 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:48,406 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:05:48,410 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:06:02,510 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:06:19,477 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:06:22,720 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:06:31,026 - __main__ - ERROR - Memory analysis error: Expecting property name enclosed in double quotes: line 6 column 1 (char 536)\n",
      "2026-01-04 15:06:31,027 - __main__ - INFO - Stored 0 new memories\n",
      "2026-01-04 15:06:31,038 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:06:31,212 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:06:31,380 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:06:31,547 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:06:31,552 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:06:45,659 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:03,155 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:06,493 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:12,791 - __main__ - INFO - Extracted 6 memories\n",
      "2026-01-04 15:07:12,951 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:12,956 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:07:13,118 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:13,123 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:07:13,292 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:13,297 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:07:13,462 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:13,467 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:07:13,636 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:13,641 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:07:13,714 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:13,717 - __main__ - INFO - Stored procedural memory for user alice-123\n",
      "2026-01-04 15:07:13,718 - __main__ - INFO - Stored 6 new memories\n",
      "2026-01-04 15:07:13,728 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:07:13,904 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:14,065 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:14,227 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:14,231 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:07:28,388 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:45,363 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:49,106 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:52,187 - __main__ - ERROR - Memory analysis error: Expecting value: line 4 column 17 (char 336)\n",
      "2026-01-04 15:07:52,188 - __main__ - INFO - Stored 0 new memories\n",
      "2026-01-04 15:07:52,198 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:07:52,370 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:52,540 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:52,720 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:07:52,723 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:08:06,881 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:08:25,175 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:08:28,437 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:08:34,874 - __main__ - INFO - Extracted 5 memories\n",
      "2026-01-04 15:08:34,875 - __main__ - ERROR - Error storing memory: 'facts'\n",
      "2026-01-04 15:08:34,876 - __main__ - ERROR - Error storing memory: 'facts'\n",
      "2026-01-04 15:08:35,041 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:08:35,045 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:08:35,214 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:08:35,219 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:08:35,385 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:08:35,390 - __main__ - INFO - Stored procedural memory for user alice-123\n",
      "2026-01-04 15:08:35,390 - __main__ - INFO - Stored 5 new memories\n",
      "2026-01-04 15:08:35,401 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:08:35,577 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Turn 6:\n",
      "  Buffer size: 259\n",
      "  Has summary: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 15:08:35,745 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:08:35,912 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:08:35,916 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:08:50,293 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:07,896 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:11,842 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:19,269 - __main__ - INFO - Extracted 6 memories\n",
      "2026-01-04 15:09:19,432 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:19,437 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:09:19,605 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:19,610 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:09:19,782 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:19,786 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:09:19,975 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:19,979 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:09:20,177 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:20,181 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:09:20,379 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:20,384 - __main__ - INFO - Stored procedural memory for user alice-123\n",
      "2026-01-04 15:09:20,384 - __main__ - INFO - Stored 6 new memories\n",
      "2026-01-04 15:09:20,396 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:09:20,600 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:20,788 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:20,971 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:20,975 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:09:35,491 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:52,702 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:09:56,346 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:02,178 - __main__ - ERROR - Memory analysis error: Expecting value: line 5 column 17 (char 553)\n",
      "2026-01-04 15:10:02,179 - __main__ - INFO - Stored 0 new memories\n",
      "2026-01-04 15:10:02,191 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:10:02,383 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:02,569 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:02,762 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:02,767 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:10:17,389 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:36,532 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:40,771 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:46,248 - __main__ - INFO - Extracted 4 memories\n",
      "2026-01-04 15:10:46,426 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:46,431 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:10:46,610 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:46,615 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:10:46,803 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:46,807 - __main__ - INFO - Stored procedural memory for user alice-123\n",
      "2026-01-04 15:10:46,996 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:47,001 - __main__ - INFO - Stored episodic memory for user alice-123\n",
      "2026-01-04 15:10:47,001 - __main__ - INFO - Stored 4 new memories\n",
      "2026-01-04 15:10:47,071 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:10:47,275 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:47,476 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:47,653 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:10:47,656 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:11:02,870 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:11:20,483 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:11:23,944 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:11:28,262 - __main__ - INFO - Extracted 3 memories\n",
      "2026-01-04 15:11:28,449 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:11:28,453 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:11:28,644 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:11:28,648 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:11:28,884 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:11:28,888 - __main__ - INFO - Stored semantic memory for user alice-123\n",
      "2026-01-04 15:11:28,889 - __main__ - INFO - Stored 3 new memories\n",
      "2026-01-04 15:11:28,899 - __main__ - INFO - Retrieving memories for user alice-123\n",
      "2026-01-04 15:11:29,111 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:11:29,301 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:11:29,483 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:11:29,487 - __main__ - INFO - Conversation threshold reached - creating summary\n",
      "2026-01-04 15:11:44,512 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:12:03,009 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "2026-01-04 15:12:07,633 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m15\u001b[39m):\n\u001b[32m     47\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMessage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Tell me something interesting about AI\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     result = \u001b[43mchat_with_production_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m5\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     56\u001b[39m         meta = result[\u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mchat_with_production_memory\u001b[39m\u001b[34m(message, user_id, session_id, buffer_size, compression_threshold)\u001b[39m\n\u001b[32m     31\u001b[39m initial_state = {\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=message)],\n\u001b[32m     33\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mretrieved_memories\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtotal_memories_stored\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m\n\u001b[32m     42\u001b[39m }\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     result = \u001b[43mproduction_memory_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     final_message = result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m]\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     50\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     51\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: final_message.content,\n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m         }\n\u001b[32m     61\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langgraph/pregel/main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langgraph/pregel/main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mextract_and_store_memories\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Analyze and extract memories\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m memories = \u001b[43mmemory_analyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze_conversation\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mai_msg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Set user_id and store\u001b[39;00m\n\u001b[32m     24\u001b[39m user_id = state[\u001b[33m\"\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mMemoryAnalyzer.analyze_conversation\u001b[39m\u001b[34m(self, user_message, ai_message)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     37\u001b[39m     chain = analysis_prompt | \u001b[38;5;28mself\u001b[39m.llm\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     response = \u001b[43mchain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser_message\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mai_message\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mai_message\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Parse JSON\u001b[39;00m\n\u001b[32m     44\u001b[39m     response_text = response.content.strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3151\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3149\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3150\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3151\u001b[39m                 input_ = context.run(step.invoke, input_, config)\n\u001b[32m   3152\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3153\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:1030\u001b[39m, in \u001b[36mChatOllama._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1024\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1025\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1028\u001b[39m     **kwargs: Any,\n\u001b[32m   1029\u001b[39m ) -> ChatResult:\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     final_chunk = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_chat_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1033\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m   1034\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m   1035\u001b[39m         message=AIMessage(\n\u001b[32m   1036\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1043\u001b[39m         generation_info=generation_info,\n\u001b[32m   1044\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:965\u001b[39m, in \u001b[36mChatOllama._chat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    956\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chat_stream_with_aggregation\u001b[39m(\n\u001b[32m    957\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    958\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    962\u001b[39m     **kwargs: Any,\n\u001b[32m    963\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    964\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterate_over_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfinal_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:1054\u001b[39m, in \u001b[36mChatOllama._iterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m   1047\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iterate_over_stream\u001b[39m(\n\u001b[32m   1048\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1049\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   1050\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1051\u001b[39m     **kwargs: Any,\n\u001b[32m   1052\u001b[39m ) -> Iterator[ChatGenerationChunk]:\n\u001b[32m   1053\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m-> \u001b[39m\u001b[32m1054\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_chat_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1056\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessage\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   1059\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1060\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/langchain_ollama/chat_models.py:952\u001b[39m, in \u001b[36mChatOllama._create_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    951\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client:\n\u001b[32m    954\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.chat(**chat_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/ollama/_client.py:181\u001b[39m, in \u001b[36mClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    178\u001b[39m   e.response.read()\n\u001b[32m    179\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43miter_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m  \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43merror\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/httpx/_models.py:929\u001b[39m, in \u001b[36mResponse.iter_lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    927\u001b[39m decoder = LineDecoder()\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/httpx/_models.py:916\u001b[39m, in \u001b[36mResponse.iter_text\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    914\u001b[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001b[32m    915\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyte_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_content\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/httpx/_models.py:897\u001b[39m, in \u001b[36mResponse.iter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    895\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/httpx/_models.py:951\u001b[39m, in \u001b[36mResponse.iter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    948\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m951\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_num_bytes_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_stream_bytes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/httpx/_client.py:153\u001b[39m, in \u001b[36mBoundSyncStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:127\u001b[39m, in \u001b[36mResponseStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpcore_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.Iterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:203\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/langgraph-tutorial/.venv/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "user_id = \"alice-123\"\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRODUCTION MEMORY SYSTEM DEMO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Conversation 1: Build initial memory\n",
    "print(\"\\n--- Building Initial Memory ---\")\n",
    "conv1 = [\n",
    "    \"Hi, my name is Alice and I'm a data scientist\",\n",
    "    \"I work with Python and specialize in machine learning\",\n",
    "    \"I'm currently learning about LangGraph and agent systems\",\n",
    "    \"My favorite ML framework is PyTorch\"\n",
    "]\n",
    "\n",
    "for msg in conv1:\n",
    "    print(f\"\\nUser: {msg}\")\n",
    "    result = chat_with_production_memory(msg, user_id)\n",
    "    if result[\"success\"]:\n",
    "        print(f\"Agent: {result['response'][:150]}...\")\n",
    "        print(f\"Stats: {result['metadata']['memories_stored']} memories stored\")\n",
    "\n",
    "# Conversation 2: Test memory recall\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- Testing Memory Recall ---\")\n",
    "\n",
    "conv2 = [\n",
    "    \"What's my name?\",\n",
    "    \"What do I do for work?\",\n",
    "    \"What ML framework do I prefer?\",\n",
    "    \"What am I currently learning about?\"\n",
    "]\n",
    "\n",
    "for msg in conv2:\n",
    "    print(f\"\\nUser: {msg}\")\n",
    "    result = chat_with_production_memory(msg, user_id)\n",
    "    if result[\"success\"]:\n",
    "        print(f\"Agent: {result['response']}\")\n",
    "        meta = result['metadata']\n",
    "        print(f\"Retrieved: {meta['memories_retrieved']} memories\")\n",
    "\n",
    "# Conversation 3: Long conversation with compression\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"--- Testing Conversation Compression ---\")\n",
    "\n",
    "for i in range(15):\n",
    "    msg = f\"Message {i+1}: Tell me something interesting about AI\"\n",
    "    result = chat_with_production_memory(\n",
    "        msg,\n",
    "        user_id,\n",
    "        buffer_size=5,\n",
    "        compression_threshold=10\n",
    "    )\n",
    "    \n",
    "    if i % 5 == 0:\n",
    "        meta = result['metadata']\n",
    "        print(f\"\\nTurn {i+1}:\")\n",
    "        print(f\"  Buffer size: {meta['buffer_size']}\")\n",
    "        print(f\"  Has summary: {meta['has_summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c6938a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph-tutorial (3.11.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
